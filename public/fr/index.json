[{"authors":["admin"],"categories":null,"content":"Après une licence en Économie et une première expérience en tant que programmeur statistique je me projette sur les métiers de la data science. J\u0026rsquo;aime particulièrement extraire de l\u0026rsquo;information à partir d\u0026rsquo;un jeu de donnée complexe en vue de fournir des éléments d\u0026rsquo;aides à la décision.\n","date":1559347200,"expirydate":-62135596800,"kind":"taxonomy","lang":"fr","lastmod":1559347200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://agailloty.rbind.io/fr/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/fr/authors/admin/","section":"authors","summary":"Après une licence en Économie et une première expérience en tant que programmeur statistique je me projette sur les métiers de la data science. J\u0026rsquo;aime particulièrement extraire de l\u0026rsquo;information à partir d\u0026rsquo;un jeu de donnée complexe en vue de fournir des éléments d\u0026rsquo;aides à la décision.","tags":null,"title":"Axel-Cleris Gailloty","type":"authors"},{"authors":null,"categories":null,"content":" Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026quot;Courses\u0026quot; url = \u0026quot;courses/\u0026quot; weight = 50  Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026quot;Docs\u0026quot; url = \u0026quot;docs/\u0026quot; weight = 50  Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"fr","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"https://agailloty.rbind.io/fr/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/fr/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":" In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"fr","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"https://agailloty.rbind.io/fr/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/fr/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":" Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"fr","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"https://agailloty.rbind.io/fr/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/fr/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":["Axel-Cleris Gailloty"],"categories":["R","visualisations"],"content":" Budget primitif du Département de Maine-et-Loire de 1998 à 2017. Le budget primitif présente les prévisions budgétaires fixées en début d\u0026rsquo;année : les recettes attendues et les montants maximum de dépenses pouvant être engagés pendant la durée de l\u0026rsquo;exercice budgétaire.\noptions(warn = -1) suppressPackageStartupMessages(library(tidyverse)) library(treemap) library(ggthemes) library(ggwordcloud) library(tm) library(SnowballC)  budget \u0026lt;- read_csv2(\u0026quot;Budget-primitif-departement.csv\u0026quot;)  Using ',' as decimal and '.' as grouping mark. Use read_delim() for more control. Parsed with column specification: cols( EXERCICE = col_double(), `LIBELLE SOUS FONCTION` = col_character(), `TYPE MVT` = col_character(), SECTION = col_character(), `NATURE MVTS` = col_character(), `BUDGET PRIMITIF` = col_double() )  glimpse(budget)  Observations: 12,198 Variables: 6 $ EXERCICE \u0026lt;dbl\u0026gt; 1998, 1998, 1998, 1998, 1998, 1998, 1998, 1... $ `LIBELLE SOUS FONCTION` \u0026lt;chr\u0026gt; \u0026quot;ABBAYE D'ASNIERES\u0026quot;, \u0026quot;ABBAYE D'ASNIERES\u0026quot;, \u0026quot;... $ `TYPE MVT` \u0026lt;chr\u0026gt; \u0026quot;DEPENSE\u0026quot;, \u0026quot;DEPENSE\u0026quot;, \u0026quot;RECETTE\u0026quot;, \u0026quot;DEPENSE\u0026quot;,... $ SECTION \u0026lt;chr\u0026gt; \u0026quot;FONCTIONNEMENT\u0026quot;, \u0026quot;FONCTIONNEMENT\u0026quot;, \u0026quot;FONCTI... $ `NATURE MVTS` \u0026lt;chr\u0026gt; \u0026quot;REEL\u0026quot;, \u0026quot;ORDRE\u0026quot;, \u0026quot;REEL\u0026quot;, \u0026quot;REEL\u0026quot;, \u0026quot;ORDRE\u0026quot;, \u0026quot;... $ `BUDGET PRIMITIF` \u0026lt;dbl\u0026gt; 8715.50, 10425.09, 0.00, 25321.79, 0.00, 0....  head(budget)   EXERCICELIBELLE SOUS FONCTIONTYPE MVTSECTIONNATURE MVTSBUDGET PRIMITIF  1998 ABBAYE D'ASNIERES DEPENSE FONCTIONNEMENT REEL  8715.50  1998 ABBAYE D'ASNIERES DEPENSE FONCTIONNEMENT ORDRE 10425.09  1998 ABBAYE D'ASNIERES RECETTE FONCTIONNEMENT REEL  0.00  1998 ABBAYE DE SAINT MAURDEPENSE FONCTIONNEMENT REEL 25321.79  1998 ABBAYE DE SAINT MAURDEPENSE FONCTIONNEMENT ORDRE  0.00  1998 ABBAYE DE SAINT MAURRECETTE FONCTIONNEMENT REEL  0.00    Le jeu de données comprend 12198 observations pour 6 colonnes.\nAttributs :\nEXERCICE : année de l\u0026rsquo;exercice budgétaire\nLIBELLE SOUS FONCTION : objet du mouvement budgétaire\nTYPE MVT : recette ou dépense\nSECTION : budget de fonctionnement ou budget d\u0026rsquo;investissement\nNATURE MVTS : réel ou ordre\nBUDGET PRMITIF : montant fixé pour la dépense ou la recette\nExploration du jeu de données Avant de visualiser ces données il convient d\u0026rsquo;explorer les colonnes du jeu de données pour les comprendre.\n# Nombres de valeurs uniques de chaque colonnes map_int(budget, function(x) length(unique(x)))  EXERCICE 20 LIBELLE SOUS FONCTION 389 TYPE MVT 2 SECTION 2 NATURE MVTS 2 BUDGET PRIMITIF 3134  Le jeu de donnée couvre les budgets du Maine-et-Loire pour une période de 20 ans. Il y a en total 389 sous fonctions (nous explorons cela). Les type de mouvement, section et nature des mouvements n\u0026rsquo;ont que deux valeurs uniques. Ces colonnes catégorisent bien les mouvements du budget.\nPar contre il n\u0026rsquo;y a que 3134 uniques budgets alors que le jeu de données comporte 12198 observations.\nEssayons de comprendre les 389 sous fonctions. La description de cette colonne nous indique qu\u0026rsquo;il s\u0026rsquo;agit de l\u0026rsquo;objet du mouvement. En y pensant un peu on peut se rendre à l\u0026rsquo;évidence que les objets du mouvement peuvent varier avec les années. Essayons tout de même d\u0026rsquo;avoir une idée des objets qui reviennent les plus souvent.\nbudget$`LIBELLE SOUS FONCTION` %\u0026gt;% sample(size = 10)  'ARTISANAT' 'DETTE RECUPERABLE' 'EQUIPEMENTS CULTURELS' 'C.I.O. SAUMUR' 'DROITS DE VOIRIE' 'ECLAIRAGE PUBLIC ET SIGNALISATION' 'ECLAIRAGE PUBLIC ET SIGNALISATION' 'RESEAU TELEPHONIQUE' 'AIDE SOCIALE FACULTATIVE A LA CHARGE DU DEPARTEMENT' 'AMENAGEMENT DE LA LOIRE'  Les objets sont divers et variés comme on peut le voir sur cet échantillon.\noptions(repr.plot.width = 8, repr.plot.height = 6, repr.plot.res = 200) budget %\u0026gt;% group_by(`LIBELLE SOUS FONCTION`) %\u0026gt;% summarize(count = n()) %\u0026gt;% arrange(desc(count)) %\u0026gt;% head(20) %\u0026gt;% ggplot(aes(x = reorder(`LIBELLE SOUS FONCTION`, count), y = count)) + geom_bar(stat = \u0026quot;identity\u0026quot;,fill = \u0026quot;lightblue\u0026quot;, col = \u0026quot;black\u0026quot;, size = 0.2) + coord_flip() + theme_economist() + labs( x = \u0026quot;Objet du mouvement\u0026quot;, y = \u0026quot;Nombre d'occurences\u0026quot;, title = \u0026quot;Occurences des 20 premiers \\n objets de mouvements du budget\u0026quot;, subtitle = \u0026quot;Toute période confondue (1998 - 2017)\u0026quot;)  Mots clés fréquents mots_cles \u0026lt;- budget$`LIBELLE SOUS FONCTION` %\u0026gt;% VectorSource %\u0026gt;% Corpus  mots_cles \u0026lt;- tm_map(mots_cles, tolower) mots_cles \u0026lt;- tm_map(mots_cles, removeWords, stopwords(\u0026quot;french\u0026quot;)) mots_cles \u0026lt;- tm_map(mots_cles, removeWords, c(\u0026quot;autre\u0026quot;, \u0026quot;autres\u0026quot;))  dtm \u0026lt;- TermDocumentMatrix(mots_cles)  dtm \u0026lt;- dtm %\u0026gt;% as.matrix() %\u0026gt;% rowSums %\u0026gt;% sort(decreasing = T) %\u0026gt;% data.frame() %\u0026gt;% rownames_to_column colnames(dtm) \u0026lt;- c(\u0026quot;mot\u0026quot;, \u0026quot;freq\u0026quot;)  wordcloud(words = dtm$mot, freq = dtm$freq, min.freq = 20, max.words = 120, random.orders = FALSE,rot.per = 0.45, colors = brewer.pal(8, \u0026quot;Dark2\u0026quot;))  Evolution du budget Les opérations réelles ont un impact direct sur la trésorerie, il s\u0026rsquo;agit d\u0026rsquo;encaissements et décaissements effectifs. Les opérations d’ordre n\u0026rsquo;ont pas de conséquences sur la trésorerie, elles ne représentent que des jeux d\u0026rsquo;écriture : - elles ne donnent lieu ni à encaissement, ni à décaissement, - elles sont retracées en dépenses et en recettes, - et sont équilibrées.\n# Convertissons EXERCICE en année. budget \u0026lt;- budget %\u0026gt;% mutate(EXERCICE = as.Date(ISOdate(EXERCICE, 1, 1)))  options(repr.plot.width = 8, repr.plot.height = 6, repr.plot.res = 200) budget %\u0026gt;% filter(`NATURE MVTS` == \u0026quot;REEL\u0026quot;) %\u0026gt;% group_by(EXERCICE, `TYPE MVT`) %\u0026gt;% summarize(budget_annuel = sum(`BUDGET PRIMITIF`)) %\u0026gt;% ggplot(aes(x = EXERCICE, y = budget_annuel)) + geom_line(linetype = 2) + facet_wrap(~`TYPE MVT`, nrow = 2, scales = \u0026quot;free_y\u0026quot;) + theme_economist() + scale_y_continuous(breaks = c(6e8, 1e9, 4e9, 5e9), label = c(\u0026quot;600M\u0026quot;, \u0026quot;1MD\u0026quot;, \u0026quot;4MD\u0026quot;, \u0026quot;5MD\u0026quot;))  Zoomons à partir de 2003 pour observer l\u0026rsquo;évolution\nbudget %\u0026gt;% filter(`NATURE MVTS` == \u0026quot;REEL\u0026quot; \u0026amp; EXERCICE \u0026gt; as.Date(\u0026quot;2003-01-01\u0026quot;)) %\u0026gt;% group_by(EXERCICE, `TYPE MVT`) %\u0026gt;% summarize(budget_annuel = sum(`BUDGET PRIMITIF`)) %\u0026gt;% ggplot(aes(x = EXERCICE, y = budget_annuel)) + geom_line(linetype = 2) + facet_wrap(~`TYPE MVT`, nrow = 2, scales = \u0026quot;free_y\u0026quot;) + theme_economist() + scale_y_continuous(breaks = c(5e8, 8e8), label = c(\u0026quot;500M\u0026quot;, \u0026quot;600M\u0026quot;))   ","date":1559347200,"expirydate":-62135596800,"kind":"page","lang":"fr","lastmod":1559347200,"objectID":"9e20cdb8592240f6e3e79f0e469d0644","permalink":"https://agailloty.rbind.io/fr/post/budget-maine-et-loire/","publishdate":"2019-06-01T00:00:00Z","relpermalink":"/fr/post/budget-maine-et-loire/","section":"post","summary":"Analyser les données d'Opendata49 sur le budget primitif du Maine-et-Loire entre 1998 et 2017","tags":["R","Econométrie"],"title":"Budget primitif du Maine-et-Loire","type":"post"},{"authors":["Axel-Cleris Gailloty"],"categories":["Python"],"content":" Adapter ses compétences aux besoins du marché du travail Introduction Aujourd\u0026rsquo;hui, plus que jamais dans l\u0026rsquo;histoire, nous vivons dans une ère où rien n\u0026rsquo;est figé. Ce qu\u0026rsquo;on connait aujourd\u0026rsquo;hui risque de changer demain dans le but d\u0026rsquo;accroître l\u0026rsquo;efficacité des facteurs de production. Il arrive des fois que les compétences enseignées par les universités où les instituts d\u0026rsquo;études supérieures soient en retard par rapport aux standards des entreprises, voilà pourquoi il est important de se mettre à jour sur les avancées technologiques afin de bien se positionner sur le marché du travail.\nDans cet article j\u0026rsquo;aimerais avoir une idée sur les compétences que demandent les entreprises aujourd\u0026rsquo;hui ainsi que le niveau de formation qu\u0026rsquo;ils exigent pour les métiers de data science / analytics.\nPour ce faire je travaillerai avec le langage Python pour scraper le site Indeed par le biais des mots clés.\n# Importons les librairies import pandas as pd from requests import get from requests.exceptions import RequestException from bs4 import BeautifulSoup  Le langage Python est riche et facile à manier pour ce genre de tâche, il contient de nombreuses librairies pour faire presque tout ce qu\u0026rsquo;on veut.\nDans cet article j\u0026rsquo;utilise les librairies Pandas, BeautifulSoup et le module requests qui fait partie de la librairie standard pour conduire ce projet.\nLe site Indeed Indeed est un site web américain spécialisé dans la mise en relation entre les entreprises et les demandeurs d\u0026rsquo;emploi. Des milliers d\u0026rsquo;offres d\u0026rsquo;emploi y sont présentes et ce site constitue un bon endroit pour scraper les offres d\u0026rsquo;emploi afin d\u0026rsquo;avoir des idées sur les compétences recherchées par les entreprises.\nLe site ressemble à ceci : Web scraping Le web scraping est un terme anglophone utilisé pour désigner l\u0026rsquo;action de fouiller des informations sur un site web.\nNous allons donc ici fouiller le site web d\u0026rsquo;Indeed pour le mot clé data science pour toutes les offres publiées pour la France.\n# Faisons une première requête, ceci équivaut à consulter la page sur un navigateur url = \u0026quot;\u0026quot;\u0026quot;https://www.indeed.fr/emplois?as_and=data+science\u0026amp;as_phr=\u0026amp;as_any=\u0026amp;as_not=\u0026amp;as_ttl=\u0026amp;as_cmp=\u0026amp;jt=all\u0026amp;st=\u0026amp;as_ src=\u0026amp;salary=\u0026amp;radius=25\u0026amp;l=France\u0026amp;fromage=any\u0026amp;limit=100\u0026amp;sort=\u0026amp;psf=advsrch \u0026quot;\u0026quot;\u0026quot; indeed = get(url)  Ici l\u0026rsquo;objet indeed enregistre les résultats bruts de recherche. Il s\u0026rsquo;agit d\u0026rsquo;une page HTML. Il nous faut filtrer cette page pour avoir l\u0026rsquo;information que nous cherchons.\nNom des entreprises soup = BeautifulSoup(indeed.text, \u0026quot;html.parser\u0026quot;)  Nous venons d\u0026rsquo;utiliser la classe BeautifulSoup pour formatter l\u0026rsquo;objet indeed en quelque chose de plus structuré.\ncompany = soup.find_all('span', {'class': 'company'})  entreprise = [] for comp in company: entreprise.append(comp.text.strip())  entreprise[0:4]  ['Total', 'Total', 'BNP Paribas Personal Finance', 'Lucky Cart']  Description des activités resume = soup.find_all('div', {'class': 'summary'})  type(resume)  bs4.element.ResultSet  description = [] for desc in resume: description.append(desc.text.strip())  description[0:4]  ['Promouvoir la data science. Gérer les pilotes data science (big data et/ou data mining), en coordination avec les autres métiers et entités....', 'PhD (or MSc/ME with equivalent experience) in Data Science, Applied Mathematics, Statistics or a related discipline....', 'Vous serez un facteur clé de la montée en compétence du Groupe en matière de Data Science, notamment par une veille technologique active sur les sujets Big Data...', '2 ans d’expérience mini en développement pour la data science avec au moins l’une des problématiques suivantes :....']  Titre de l\u0026rsquo;offre title = soup.find_all(\u0026quot;div\u0026quot;, {\u0026quot;class\u0026quot;: \u0026quot;title\u0026quot;}) titre = [] for t in title: titre.append(t.text.strip())  titre[0:4]  ['Data scientist H/F', 'Data Science and Artificial Intelligence Research Scientist', 'DATA SCIENTIST H/F', 'Data Engineer']  Lieu du travail location = soup.find_all(\u0026quot;span\u0026quot;, {\u0026quot;class\u0026quot;: \u0026quot;location\u0026quot;}) lieu = [] for loc in location: lieu.append(loc.text.strip())  Dans les codes HTML du site, le lieu apparaît à la fois sous les tags span et div. Donc il faut prendre en compte les deux.\nEt heureusement, les tags div sont les 8 derniers.\nlocation_div = soup.find_all(\u0026quot;div\u0026quot;, {\u0026quot;class\u0026quot;: \u0026quot;location\u0026quot;}) for loc in location_div: lieu.append(loc.text.strip())  lieu[0:4]  ['Paris (75)', 'Paris (75)', 'Chilly-Mazarin (91)', 'Vélizy-Villacoublay (78)']  Data frame Maintenant que nous avons pu trouver les informations qui nous intéressent, nous pouvons maintenant assembler ces données dans un Data frame afin de les analyser.\nlist(map(len, [titre, entreprise, lieu, description]))  [59, 59, 59, 59]  Nous avons donc toutes les 58 locations. Créons notre objet Pandas.\ndata_science = pd.DataFrame({\u0026quot;titre\u0026quot;: titre, \u0026quot;entreprise\u0026quot;: entreprise, \u0026quot;lieu\u0026quot;:lieu, \u0026quot;description\u0026quot;: description})  data_science.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    titre entreprise lieu description     0 Data scientist H/F Total Paris (75) Promouvoir la data science. Gérer les pilotes ...   1 Data Science and Artificial Intelligence Resea... Total Paris (75) PhD (or MSc/ME with equivalent experience) in ...   2 DATA SCIENTIST H/F BNP Paribas Personal Finance Chilly-Mazarin (91) Vous serez un facteur clé de la montée en comp...   3 Data Engineer Lucky Cart Vélizy-Villacoublay (78) 2 ans d’expérience mini en développement pour ...   4 Ingénieur Big Data H/F Naval Group Toulon (83) Concevoir et développer des solutions pour les...     Décrivons le jeu de données data_science.describe()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    titre entreprise lieu description     count 59 59 59 59   unique 56 48 25 57   top Data Science and Artificial Intelligence Resea... In-Team Paris (75) PhD (or MSc/ME with equivalent experience) in ...   freq 2 4 23 2     Voici donc un simple tableau qui nous donne les statistiques descriptives du jeu de donnée.\nSur les 5ç offres, 23 se trouvent à Paris (75). Regardons cela visuellement\n%matplotlib inline import matplotlib.pyplot as plt # reglages plt.rcParams[\u0026quot;figure.figsize\u0026quot;] = [12,6] plt.rcParams[\u0026quot;figure.dpi\u0026quot;] = 300 plt.style.use(\u0026quot;ggplot\u0026quot;)  data_science[\u0026quot;lieu\u0026quot;].value_counts().plot(kind = \u0026quot;bar\u0026quot;) plt.title(\u0026quot;Nombre d'offres d'emploi par lieu\u0026quot;);  Le constat est clair : presque toutes les offres d\u0026rsquo;emploi qui parlent de data science sont concentrées en région parisienne. Si quelqu\u0026rsquo;un se prépare donc exercer le métier de data scientist il faudra donc considérer à postuler dans les entreprises parisiennes.\nEt les entreprises ?\ndata_science[\u0026quot;entreprise\u0026quot;].value_counts().plot(kind = \u0026quot;bar\u0026quot;);  Les offres d\u0026rsquo;emploi par entreprise ne suivent aucune tendance. Il y a juste qu\u0026rsquo;à remarquer que la plupart de ces entreprises se situent en région parisienne.\nAnalyse textuelle des titres et des descriptions data_science[\u0026quot;titre\u0026quot;].head()  0 Data scientist H/F 1 Data Science and Artificial Intelligence Resea... 2 DATA SCIENTIST H/F 3 Data Engineer 4 Ingénieur Big Data H/F Name: titre, dtype: object  Les titres ne suivent pas une structure définie, il nous faut d\u0026rsquo;abord les normaliser : enlever les ponctuations, les majuscules etc\u0026hellip;\nfrom string import punctuation  print(punctuation)  !\u0026quot;#$%\u0026amp;'()*+,-./:;\u0026lt;=\u0026gt;?@[\\]^_`{|}~  words = \u0026quot; \u0026quot;.join(titre).translate(str.maketrans(\u0026quot;\u0026quot;,\u0026quot;\u0026quot;, punctuation)).lower()  from wordcloud import WordCloud, STOPWORDS  plt.rcParams[\u0026quot;figure.dpi\u0026quot;] = 300 wc = WordCloud(background_color=\u0026quot;white\u0026quot;, width= 800, height= 400).generate(words) plt.imshow(wc) plt.axis(\u0026quot;off\u0026quot;);  Dans les titres on peut déjà avoir une idée de ce que cherchent les entreprises. Il y a plusieurs mots-clés qui reviennent dans les offres d\u0026rsquo;emploi.\nLes expressions data science et data scientist reviennent souvent mais n\u0026rsquo;ont pas réellement de sens puisque la data science est large. Pour avoir une idée des compétences et savoirs-faire, penchons nous sur les descriptions.\nPour n\u0026rsquo;avoir que les mots clés, il nous faut enlever les mots vides de la langue française. Ce sont des mots comme \u0026ldquo;de\u0026rdquo;, \u0026ldquo;le\u0026rdquo;, \u0026ldquo;la\u0026rdquo;, \u0026ldquo;en\u0026rdquo;. Ces mots existent pour embellir la langue et donner du sens dans une phrase or notre but est de chercher les mots clés.\nfrom nltk.corpus import stopwords  mots_vides = stopwords.words(\u0026quot;french\u0026quot;)  update = [\u0026quot;les\u0026quot;, \u0026quot;data\u0026quot;, \u0026quot;science\u0026quot;, \u0026quot;data science\u0026quot;, \u0026quot;or\u0026quot;, \u0026quot;tant\u0026quot;, \u0026quot;d'une\u0026quot;, \u0026quot;scientist\u0026quot;] for word in update: mots_vides.append(word)  desc = data_science[\u0026quot;description\u0026quot;].str.lower().str.translate(str.maketrans(\u0026quot; \u0026quot;,\u0026quot; \u0026quot;, punctuation))  desc = \u0026quot; \u0026quot;.join(desc).split()  desc = \u0026quot; \u0026quot;.join([word for word in desc if word not in mots_vides])  desc_wc = WordCloud(background_color= \u0026quot;white\u0026quot;, width = 1200, height = 600).generate(desc) plt.imshow(desc_wc, interpolation='bilinear') plt.axis(\u0026quot;off\u0026quot;);   ","date":1558828800,"expirydate":-62135596800,"kind":"page","lang":"fr","lastmod":1558828800,"objectID":"8318951f98f41f09240028a75912092b","permalink":"https://agailloty.rbind.io/fr/post/webscraping/","publishdate":"2019-05-26T00:00:00Z","relpermalink":"/fr/post/webscraping/","section":"post","summary":"Adapter ses compétences aux besoins du marché du travail en scrapant les offres d'emploi","tags":["Python","webscrape","analysis"],"title":"Adapter ses compétences aux besoins du marché du travail","type":"post"},{"authors":["Axel-Cleris Gailloty"],"categories":["R","Econométrie"],"content":" Les modélisations ARCH et GARCH Introduction Un des avantages principaux qu\u0026rsquo;une série temporelle nous donne est de garder le record de l\u0026rsquo;évolution d\u0026rsquo;une variable, d\u0026rsquo;un grandeur dans le temps. Très vite en économie on a compris l\u0026rsquo;importance de créer des variables fiables dont on peu suivre l\u0026rsquo;évolution sur une période de temps (ex: taux de chômage, taux d\u0026rsquo;inflation, etc\u0026hellip;) et observer à partir des valeurs que cette variable prend à différentes périodes du temps pour et faire des prévisions. L\u0026rsquo;exemple le plus flagrant qu\u0026rsquo;on observe dans la vie de tous les jours c\u0026rsquo;est celle de la saisonalité. Prenons par exemple l\u0026rsquo;exemple d\u0026rsquo;évolution sur une année du nombre de vol d\u0026rsquo;avion : ce qu\u0026rsquo;on observe c\u0026rsquo;est qu\u0026rsquo;il existe des périodes où il y a un pic des vols, ces périodes correspondent souvent aux vacances où les personnes voyagent. Si donc on enregistre les données sur plusieurs années non seulement on est en mesure de détecter les tandances mais aussi d\u0026rsquo;observer les fluctuations et à partir d\u0026rsquo;elles et faire des prévisions sur les périodes à venir. C\u0026rsquo;est cette question qui est au coeur des méthodes d\u0026rsquo;estimation de l\u0026rsquo;économétrie de la finance où on travaille sur souvent de longues périodes de temps.\nC\u0026rsquo;est quoi ARCH et GARCH ? Ces acronymes un peu spéciaux sont en anglais et signifient pour l\u0026rsquo;ARCH : AutoRegressive Conditional Heteroskedacity et le GARCH pour Generalised ARCH. Il s\u0026rsquo;agit d\u0026rsquo;un type de modèle qui permet d\u0026rsquo;estimer et prévoir la volatilité du prix d\u0026rsquo;un action ou d\u0026rsquo;un rendement à court terme en se basant sur les valeurs que prennent ces prix ou rendements quelques périodes de temps auparavant.\nLe terme AutoRegressive signifie qu\u0026rsquo;on regresse le modèle à partir de lui-même, c\u0026rsquo;est-à-dire que les variables exogènes sont les retards de la variable endogène à l\u0026rsquo;ordre q. Le terme Heteroskedacity accentue le fait que la variance n\u0026rsquo;est constante dans le temps. Nous disons qu\u0026rsquo;il y a hétéroscédasticité si dans la série temporelle il y a une ou des sous-périodes dont la variance est différente de la variance des autres périodes. Dans les séries financières, souvent la variation de la variance est souvent causée par un évennement particulier qui peut surgir sur le marché, donc si notre variance est hétéroscédastique, elle l\u0026rsquo;est conditionnellement aux intéractions du marché d\u0026rsquo;où le terme conditional hetereoskedacity.\nLa grande nouveauté qu\u0026rsquo;apporte les modèles de types ARCH est que non seulement il prend en compte la valeur de la variable à N-p périodes mais aussi le changement dans la valeur de la variance à N-p, ce faisant il améliore grandement la prévision de la volatilité.\nPrincipe du modèle ARCH Le cas le plus simple des modèle ARCH est le modèle ARCH(1) qui ne prend en compte que le changement d\u0026rsquo;ordre 1 de la variance de la série temporelle. Formellement elle se présente sous cette forme $$\\epsilon_t = \\omega_t* \\sqrt {\\alpha_0 + \\alpha1*\\epsilon{t-1}{^2}}$$ L\u0026rsquo;expression sous la parenthèse représente la variance conditionnelle. Pour simplifier cette écriture on peut dire que le processus ARCH(1) s\u0026rsquo;écrit de la manière suivante: $$\\epsilon = \\sigma.\\omega_t$$ où $$\\sigma$$ est l\u0026rsquo;expression obtenue plus haut.\nPrincipe des modèles GARCH Les modèles GARCH suivent le même principe que le modèle ARCH mais ajoute un second membre à l\u0026rsquo;équation qui est la moyenne mobile d\u0026rsquo;ordre q. Les modèles GARCH s\u0026rsquo;écrivent de manière globale sous cette forme: $$\\epsilon{^2}_t = \\sum\\omega_t* \\sqrt {\\alpha_0 + \\alpha1*\\epsilon{t-1}{^2}}$$\n$$\\sigma^2_t = \\alpha0 + \\sum\\limits{i=1}^q \\alphai\\epsilon{^2}{ti} + \\sum\\limits{i=1}^p \\betaj\\sigma{^2}{t_j}$$\nObjet du projet Dans ce projet, j\u0026rsquo;aimerais analyser la volatilité des actifs financier (le prix de l\u0026rsquo;action) de l\u0026rsquo;entreprise américaine IBM. Je lirai les données sur une période de 15 ans et j\u0026rsquo;appliquerai différentes familles de modèles GARCH sur les données dans le but de voir si on peut réellement prévoir la volatilité du cours de l\u0026rsquo;action avec le temps.\nLe package quantmod Pour ce projet j\u0026rsquo;utiliserai l\u0026rsquo;excellent package quantmod pour lire directement les informations financières concernant IBM dont j\u0026rsquo;ai besoin.\n# Chargement les librairies library(tidyverse) library(quantmod)  Lecture des informations getSymbols(Symbols = \u0026quot;IBM\u0026quot;, src = \u0026quot;yahoo\u0026quot;, from = as.Date(\u0026quot;2004-01-01\u0026quot;), to = as.Date(\u0026quot;2019-03-01\u0026quot;))  Je viens de lire les données d\u0026rsquo;IBM directement à partir de Yahoo Finance pour la période du 1er janvier 2004 au 1er mars 2019. En lisant ces données, R crée automatiquement un objet IBM dans notre environnement de travail. On peut voir quel type d\u0026rsquo;objet c\u0026rsquo;est.\nclass(IBM)  'xts' 'zoo'  L\u0026rsquo;objet hérite de deux classesxts et zoo qui sont des classes pour séries temporelles. Le package quantmod vient avec une série de méthodes (fonctions) qui pour analyser automatiquement les objets de ces classes. Nous allons les explorer, mais avant, voyons comment la base de donnée se présente.\nhead(IBM)   IBM.Open IBM.High IBM.Low IBM.Close IBM.Volume IBM.Adjusted 2004-01-02 92.86 93.05 91.20 91.55 5327800 64.01743 2004-01-05 92.00 93.09 92.00 93.05 5276300 65.06632 2004-01-06 92.20 93.19 92.14 93.06 4380000 65.07331 2004-01-07 93.14 93.38 92.47 92.78 4927600 64.87749 2004-01-08 93.21 93.21 92.03 93.04 6179800 65.05934 2004-01-09 91.75 92.35 91.00 91.21 7930900 63.77967  La base de donné commence à partir du 2 janvier 2004 alors qu\u0026rsquo;on avait demandé à partir du 1er. Des raisons particulières? Oui c\u0026rsquo;est le nouvel an, et ce n\u0026rsquo;était pas un business day. On a les mêmes observations entre les 2 et 5 de cette même année.\ndim(IBM)  3815 6  On a 3815 observations et 6 variables.\nVisualisation La meilleure manière de comprendre la tendance d\u0026rsquo;une série financière c\u0026rsquo;est de l\u0026rsquo;observer graphiquement, nous allons dans cette partie visualiser la série sur toute la période considérée et dégager une première impression.\noptions(repr.plot.res = 300, repr.plot.height = 3) # paramètres graphiques, res pour résolution  plot(IBM[, \u0026quot;IBM.Open\u0026quot;], main = \u0026quot;Prix d'ouverture de l'action IBM\u0026quot;, col = \u0026quot;lightblue\u0026quot;)  On a pris l\u0026rsquo;avantage des classes xts et zoo pour facilement visualiser cette série temporelle avec la fonction générique plot().\nSur la période considérée on observe une nette augmentation du prix d\u0026rsquo;ouverture de l\u0026rsquo;action IBM. Ce prix est à son plus bas niveau entre 2008 et 2009, on peut tout de suite voir que la crise des subrpimes a quelque chose à y voir. Les prix des actions sont très volatiles car ils dépendent TOTALEMENT du comportement des acteurs qui interviennent sur le marché financier.\nOn peut également représenter toutes les prix sur un même graphique pour voir comment ils évoluent. Y a-t-il des différences entre les prix d\u0026rsquo;ouverture, de fermeture\u0026hellip;\noptions(repr.plot.res = 300, repr.plot.height = 4.4) # paramètres graphiques, res pour résolution plot.xts(IBM[,1:4],legend.loc = \u0026quot;left\u0026quot;, main = \u0026quot;Prix de l'action IBM\u0026quot;, col = rainbow(4))  La plupart du temps ces valeurs se superposent, et la tendance générale est la même pour tous les différents prix.Il y a toutefois une nette différence entre les prix haut et bas. Regardons les dans les détails\noptions(repr.plot.res = 300, repr.plot.height = 4.4) # paramètres graphiques, res pour résolution plot.xts(IBM[,2:3],legend.loc = \u0026quot;left\u0026quot;, main = \u0026quot;Prix plus haut et plus bas\u0026quot;, col = rainbow(n= 2))  Intéressons-nous maintenant au volume\nplot(IBM[, \u0026quot;IBM.Volume\u0026quot;]/1000000, col = \u0026quot;lightblue\u0026quot;, main = \u0026quot;Volume d'actions échangées en millions\u0026quot;)  Les volumes échangés fluctuent les plus. Il n\u0026rsquo;y a pas une tendance à la hausse comme on a pu l\u0026rsquo;observer pour les prix. Le volume échangé est plus aléatoire.\nLa librairie quantmod a une série de fonctions pour représenter les visuellement les graphiques dont candleChart.\noptions(repr.plot.res = 300, repr.plot.height = 5) # paramètres graphiques, res pour résolution candleChart(IBM, type = \u0026quot;line\u0026quot;, theme = \u0026quot;white\u0026quot;, show.grid = T)  Modélisations Il existe de nombreux packages pour faire la modélisation GARCH avec R. Ne sachant pas moi-même le meilleur à prendre, je me suis inspiré de la vidéo de Ralph Becker que vous pouvez regarder ici.\nOn fera la modélisation avec les packages rugarch et rmgarch respectivement pour Univariate GARCH et Multivariate GARCH.\n# Installons les packages #install.packages(c(\u0026quot;rugarch\u0026quot;, \u0026quot;rmgarch\u0026quot;))  library(rugarch) library(rmgarch)  Les documentations concernant ces deux packages se trouvent sur les sites suivants : rugarch et rmgarch. La syntaxe de ces packages diffère un peu des autres fonctions R.\nGARCH Univarié Nous n\u0026rsquo;allons utiliser que le GARCH univarié car nous n\u0026rsquo;avons pas d\u0026rsquo;exogènes à proprement parler à inclure dans le modèle. Pour un GARCH multivarié il faudra lire les données de plusieurs entreprises en même temps\nuniv_garch = ugarchspec() # On commence par instancier l'objet ugarch sans y ajouter de paramètres  univ_garch  *---------------------------------* * GARCH Model Spec * *---------------------------------* Conditional Variance Dynamics ------------------------------------ GARCH Model : sGARCH(1,1) Variance Targeting : FALSE Conditional Mean Dynamics ------------------------------------ Mean Model : ARFIMA(1,0,1) Include Mean : TRUE GARCH-in-Mean : FALSE Conditional Distribution ------------------------------------ Distribution : norm Includes Skew : FALSE Includes Shape : FALSE Includes Lambda : FALSE  Ce sont les configurations par défaut. sGARCH signifie standard GARCH c\u0026rsquo;est le GARCH simple car il existe une grande variété de modèles GARCH. Faisons une première estimation avec ces spécifications.\nmod \u0026lt;- ugarchfit(spec = univ_garch, data = IBM[, \u0026quot;IBM.Open\u0026quot;])  On peut afficher les résultats directement en exécutant l\u0026rsquo;objet mod mais les résultats sont très nombreux et risquent de prendre de la place sur l\u0026rsquo;écran. Donc nous n\u0026rsquo;allons sélectionner que ce qui nous intéresse dans un modèle GARCH, c\u0026rsquo;est-à-dire la variance conditionnelle $ \\sigma^2 $\nL\u0026rsquo;objet mod qu\u0026rsquo;on a créé contient de familles de résultats: les résultats du modèle et les résultats d\u0026rsquo;estimation. Pour accéder à l\u0026rsquo;un ou l\u0026rsquo;autre de ces résultats on utilise @ après mod.\n# affichons les noms des informations contenues dans ces deux objets print(\u0026quot;Objets contenus dans model@fit\u0026quot;) names(mod@fit) print(\u0026quot;Objets contenus dans model@model\u0026quot;) names(mod@model)  [1] \u0026quot;Objets contenus dans model@fit\u0026quot;  'hessian' 'cvar' 'var' 'sigma' 'condH' 'z' 'LLH' 'log.likelihoods' 'residuals' 'coef' 'robust.cvar' 'A' 'B' 'scores' 'se.coef' 'tval' 'matcoef' 'robust.se.coef' 'robust.tval' 'robust.matcoef' 'fitted.values' 'convergence' 'kappa' 'persistence' 'timer' 'ipars' 'solver'  [1] \u0026quot;Objets contenus dans model@model\u0026quot;  'modelinc' 'modeldesc' 'modeldata' 'pars' 'start.pars' 'fixed.pars' 'maxOrder' 'pos.matrix' 'fmodel' 'pidx' 'n.start'  # Regardons les coefficients d'estimation mod@fit$matcoef   EstimateStd. Errort valuePr(\u0026gt;|t|)  mu92.8716302801.375150510  67.535611 0.000000e+00 ar1 0.9997454780.000533952 1872.351017 0.000000e+00 ma1-0.0243733500.016509271  -1.476343 1.398518e-01 omega 0.0073600530.001237242  5.948758 2.701844e-09 alpha1 0.0240243980.001488674  16.138116 0.000000e+00 beta1 0.9746045610.002044855  476.612995 0.000000e+00   Nous avons dans ce tableau se trouvent les informations de l\u0026rsquo;estimation. Dans la spécification du modèle nous avons choisi un modèle GARCH simple garch(p=1,q=1) c\u0026rsquo;est ce que représentent les $\\alpha_1$ et $\\beta_1$ dont on a les valeurs.\nOn peut présenter ce modèle comme cela : $$ \\mu + AR1 + MA1 + \\omega + \\alpha_1 + \\beta_1$$\nVariance conditionnelle estimée Nous allons créer un data frame dans lesquel nous sauvons ces résultats pour les visualiser\nvariance \u0026lt;- xts(mod@fit$var, order.by = as.Date(index(IBM))) names(variance) \u0026lt;- \u0026quot;variance\u0026quot; head(variance)   variance 2004-01-02 2.982531 2004-01-05 2.914152 2004-01-06 2.865286 2004-01-07 2.800649 2004-01-08 2.758303 2004-01-09 2.695823  plot(variance, main = \u0026quot;variance conditionnelle estimée\u0026quot;, col = \u0026quot;lightblue\u0026quot;)  Prévisions sur le passé Nous allons comparer les performances de notre modèle à prédire les valeurs du passé. Nous construirons un graphique sur lequel les deux séries se superposent pour observer les différences\nprev \u0026lt;- cbind(c(IBM[, \u0026quot;IBM.Open\u0026quot;]), mod@fit$fitted.values) names(prev) \u0026lt;- c(\u0026quot;valeur.observee\u0026quot;, \u0026quot;valeur.predite\u0026quot;) head(prev)   valeur.observee valeur.predite 2004-01-02 92.86 92.87163 2004-01-05 92.00 92.86029 2004-01-06 92.20 92.02119 2004-01-07 93.14 92.19581 2004-01-08 93.21 93.11692 2004-01-09 91.75 93.20764  help(plot.xts)  plot.xts(prev, col = c(\u0026quot;lightblue\u0026quot;, \u0026quot;red\u0026quot;), legend.loc= \u0026quot;left\u0026quot;, lwd = 1)  La performance globale du modèle est très bonne pour la prévision dans le passé. Les modèles GARCH sont généralement très bons lorsque la période considérée est relativement courte.\nPour avoir une idée plus claire de cette différence, nous pouvons représenter graphiquement les résidus et le carré de ces résidus.\noptions(repr.plot.res = 300, repr.plot.height = 3) # paramètres graphiques, res pour résolution resid \u0026lt;- xts(mod@fit$residuals, order.by = as.Date(index(IBM))) plot.xts(resid, main = \u0026quot;Résidus du modèle\u0026quot;, col = \u0026quot;lightblue\u0026quot;) resid2 \u0026lt;- xts(mod@fit$residuals^2, order.by = as.Date(index(IBM))) plot.xts(resid2, main = \u0026quot;Carré des résidus du modèle\u0026quot;, col = \u0026quot;lightblue\u0026quot;)  Les rendements Le calcul du rendement est simple, c\u0026rsquo;est la variation du prix de fermeture de l\u0026rsquo;action sur la période considérée. la formule est la suivante: $$ R = \\frac {prixt - prix{t-1}} {prix_{t-1}}$$ Généralement on calcule le rendement journalier. quantmodintègre une série de fonctions qui nous permettent de calculer les rendements dont dailyReturn qui calcule le rendement journalier.\nrIBM \u0026lt;- dailyReturn(IBM) head(rIBM)   daily.returns 2004-01-02 -0.0141072365 2004-01-05 0.0163844888 2004-01-06 0.0001074154 2004-01-07 -0.0030088008 2004-01-08 0.0028023497 2004-01-09 -0.0196689809  options(repr.plot.res = 300, repr.plot.height = 3) # paramètres graphiques, res pour résolution plot(rIBM, main = \u0026quot;Rendement journalier de l'action IBM\u0026quot;, col = \u0026quot;lightblue\u0026quot;)  Modélisation du rendement Pour le rendement, je choisis les spécifications suivantes: j\u0026rsquo;utilise un \u0026ldquo;eGARCH\u0026rdquo;, ce choix est juste aléatoire, couplé avec un retard d\u0026rsquo;ordre 3 et d\u0026rsquo;une moyenne mobile 3 aussi. La moyenne est une moyenne ARCH-in-mean.\nreturn_garch \u0026lt;- ugarchspec(variance.model = list(model = \u0026quot;eGARCH\u0026quot;, garchOrder = c(3,3)), mean.model = list(archm = 3))  fit_return \u0026lt;- ugarchfit(spec = return_garch, data = rIBM)  Résultats fit_return@fit$matcoef   EstimateStd. Errort valuePr(\u0026gt;|t|)  mu 0.00033731780.0002535284  1.3304929 1.833559e-01  ar1-0.28014958730.0209793094 -13.3536134 0.000000e+00  ma1 0.25759855430.0213733469  12.0523265 0.000000e+00  archm-0.00566316580.0277292577  -0.2042307 8.381732e-01  omega-0.95736234750.1969173808  -4.8617463 1.163547e-06  alpha1-0.05995710730.0196478443  -3.0515870 2.276350e-03  alpha2-0.08761036320.0200375304  -4.3723134 1.229369e-05  alpha3 0.01092738560.0209578236  0.5213989 6.020889e-01  beta1 0.13152296960.0920725229  1.4284714 1.531562e-01  beta2 0.27116354200.0662792739  4.0912268 4.290972e-05  beta3 0.48587715580.0371937507  13.0634084 0.000000e+00  gamma1 0.21837131180.0307523642  7.1009601 1.239009e-12  gamma2 0.16530534080.0385267970  4.2906588 1.781438e-05  gamma3-0.02599197280.0309438681  -0.8399717 4.009243e-01    Variance conditionnelle return_var \u0026lt;- xts(fit_return@fit$var, order.by = as.Date(index(rIBM))) plot(return_var, main = \u0026quot;Variance conditionnelle du rendement\u0026quot;, col = \u0026quot;lightblue\u0026quot;)     Prévision sur le futur Nous pouvons faire une prévision des prix futurs avec la fonction ugarchforecast\nugarchforecast(fitORspec = mod, n.ahead = 10)  *------------------------------------* * GARCH Model Forecast * *------------------------------------* Model: sGARCH Horizon: 10 Roll Steps: 0 Out of Sample: 0 0-roll forecast [T0=2019-02-28]: Series Sigma T+1 138.8 1.919 T+2 138.8 1.920 T+3 138.7 1.921 T+4 138.7 1.921 T+5 138.7 1.922 T+6 138.7 1.922 T+7 138.7 1.923 T+8 138.7 1.924 T+9 138.7 1.924 T+10 138.7 1.925  mean(IBM[, \u0026quot;IBM.Open\u0026quot;])  137.557446927392\nNous venons de faire une prévision sur 10 jours futurs.\n","date":1557014400,"expirydate":-62135596800,"kind":"page","lang":"fr","lastmod":1557014400,"objectID":"e03319d9efa8d6de7fc8a18fefff5a87","permalink":"https://agailloty.rbind.io/fr/post/arch/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/fr/post/arch/","section":"post","summary":"Les modélisations avec R","tags":["R","Econométrie"],"title":"Modélisations ARCH et GARCH","type":"post"},{"authors":["Axel-Cleris Gailloty"],"categories":["Python","Econométrie"],"content":" J\u0026rsquo;aimerais dans cet article réaliser une étude économétrique à partir d\u0026rsquo;une base de donnée regroupant des indicateurs économétriques sur la France de 1962 à 2012. Il sera question de nettoyer la base pour la préparer à la visualisation des séries temporelles, pour réaliser des statistiques descriptives, analyser les correlations entre les variables de la base de donnée.\n# importation des librairies %matplotlib inline import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt  # Paramètres graphiques plt.rcParams[\u0026quot;figure.figsize\u0026quot;] = [12,6] plt.rcParams[\u0026quot;figure.dpi\u0026quot;] = 150 plt.style.use(\u0026quot;fivethirtyeight\u0026quot;)  base = pd.read_excel(\u0026quot;DATABASE1963-2012.xls\u0026quot;)  Regardons comment se présente la base\nbase.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    OBS UN Unnamed: 2 CPC DAT DLT DP DSPS DUC GPG ... TDE TE TINFL TPRES TR TSP W XPX YPY TEF     0 1 1 1963-1 274.252399 1859.226699 40.0 14.999408 4.575718 82.798301 103.402718 ... 25.207117 19.547539 5.160079 32.019116 6.465074 -0.142149 0.759338 33.289599 476.046181 8.815188   1 2 1 1963-2 279.979818 1876.450261 40.0 16.075849 1.588363 84.328672 105.287914 ... 25.919505 18.990496 4.982553 32.541230 6.482865 0.302659 0.767427 33.197693 486.892013 7.582511   2 3 1 1963-3 284.897175 1887.905376 40.0 16.901766 -0.326363 85.182034 107.088900 ... 26.345618 18.640453 4.705747 32.955774 6.457475 0.616460 0.776522 33.452026 496.462215 6.721496   3 4 1 1963-4 289.130436 1894.530152 40.0 17.505219 -1.354848 85.477076 108.809399 ... 26.518268 18.465213 4.357520 33.274753 6.402607 0.816080 0.786548 33.995435 504.953332 6.172891   4 5 1 1964-1 292.752877 1897.266903 40.0 17.909676 -1.687237 85.340560 110.427187 ... 26.473363 18.431377 3.972974 33.507235 6.333507 0.917762 0.797246 34.755299 512.449470 5.879116    5 rows × 39 columns\n Il s\u0026rsquo;agit d\u0026rsquo;une base de donnée économique regroupant des variables macroéconomiques sur la France. Nous n\u0026rsquo;allons pas détailler tous les noms des variables, mais nous tâcherons d\u0026rsquo;expliquer les variables que nous utiliserons pour notre analyse.\nStatistiques descriptives base.shape  (200, 39)  Notre base de donnée est constituée de 200 observations et de 39 variables.\nbase.describe()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    OBS UN CPC DAT DLT DP DSPS DUC GPG H ... TDE TE TINFL TPRES TR TSP W XPX YPY TEF     count 200.000000 200.0 200.000000 200.000000 200.000000 200.000000 200.000000 200.000000 200.000000 200.000000 ... 200.000000 200.000000 200.000000 200.000000 200.000000 200.000000 200.000000 200.000000 200.000000 200.000000   mean 100.500000 1.0 667.976435 1581.860187 38.320205 495.842394 -1.434067 84.338210 289.095131 131.821682 ... 40.329048 16.694269 4.666106 40.115857 7.232212 -2.400353 8.899128 232.055791 1204.883787 5.687504   std 57.879185 0.0 219.857030 161.707290 2.026990 519.926384 8.667881 2.804489 105.181059 13.475607 ... 21.782975 2.796075 3.852316 3.975054 3.768669 2.064637 5.940982 153.442257 401.474897 1.714773   min 1.000000 1.0 274.252399 1378.022919 34.284686 14.999408 -18.214287 72.446980 103.402718 114.835243 ... 12.693384 10.877190 -0.384555 32.019116 0.167617 -8.265285 0.759338 33.197693 476.046181 0.323111   25% 50.750000 1.0 486.067715 1426.873623 35.244837 36.631097 -7.761538 83.108515 199.082330 118.906135 ... 20.953342 14.739361 1.900946 35.800408 3.723566 -3.377893 2.677703 105.780221 884.587609 4.991652   50% 100.500000 1.0 678.466020 1529.531753 39.000379 292.753615 -1.415239 84.607695 301.039485 127.460979 ... 33.357799 15.936473 2.858203 41.823950 7.539372 -2.512027 9.335488 173.734371 1189.764734 6.027491   75% 150.250000 1.0 854.507537 1710.212066 39.999998 825.963580 4.709407 86.007016 374.805154 142.517672 ... 59.287390 19.152971 6.700211 43.245983 10.365802 -0.832687 14.051128 409.184189 1586.473061 6.670885   max 200.000000 1.0 1013.837952 1897.266903 40.179177 1859.217287 24.796444 90.185537 457.730691 158.105575 ... 91.179840 22.687047 14.242208 45.382268 15.817231 0.942276 18.945301 518.429723 1812.921288 9.400338    8 rows × 38 columns\n On remarque qu\u0026rsquo;il y a une colonne qui ne nous dit pas grand chose sur les données. C\u0026rsquo;est la colonne UN qui prend partout 1.\nA la lecture de la base il y avait une colonne appélée Unnamed. Nous allons essayer de la corriger\nSignification des libéllés PY PRIX DE LA PRODUCTION (PIB)\nPINV PRIX DE L’INVESTISSEMENT\nPM PRIX DES IMPORTATIONS\nPX PRIX DES EXPORTATIONS\nPG PRIX DES DEPENSES PUBLIQUES\nPDI PRIX DE LA DEMANDE INTERIEURE\nPC PRIX DE LA CONSOMMATION\nL EMPLOI TOTAL\nPAP POPULATION ACTIVE\nPOP POPULATION TOTALE\nYPY PIB en volume INVP FBCF en volume\nCPC CONSOMATION DES MENAGES en volume\nXPX EXPORTATIONS en volume\nMPM IMPORTATIONS en volume\nRD REVENU DISPONIBLE en volume\nGPG CONSOMMATION DE L’ETAT (dépenses publiques) en volume\nK CAPITAL\nDSPS VARIATION DE STOCKS en volume\nTCHO TAUX DE CHOMAGE en %\nTINFL TAUX D’INFLATION en %\nTPRES TAUX DE PRELEVEMENT OBLIGATOIRE en % du PIB\nTCOT TAUX DE COTISATION SOCIALES DES MENAGES en %\nDUC DEGRES D’UTILISATION DES CAPACITE DE PRODUCTION en %\nDAT DUREE ANNUELLE DU TRAVAIL en heure/personne\nDLT DUREE LEGALE DU TRAVAIL en heure\nH DUREE MENSUELLE DU TRAVAIL en heure/personne\nDP DETTE PUBLIQUE en valeur\nSP SOLDE PUBLIQUE en valeur\nTDE DETTE PUBLIQUE en % du PIB\nTE TAUX D’EPARGNE DES MENAGES\nTSP SOLDE PUBLIQUE (DEFICIT) en % du PIB\nTR EURIBOR 3 mois W SALAIRE\nbase[\u0026quot;Unnamed: 2\u0026quot;].head()  0 1963-1 1 1963-2 2 1963-3 3 1963-4 4 1964-1 Name: Unnamed: 2, dtype: object  base[\u0026quot;Unnamed: 2\u0026quot;].tail()  195 2011-T4 196 2012-T1 197 2012-T2 198 2012-T3 199 2012-T4 Name: Unnamed: 2, dtype: object  Il s\u0026rsquo;agit de 200 périodes de temps commençant du premier semestre de 1964 au quatrième semestre de l\u0026rsquo;année 2012. Ce type de donnée n\u0026rsquo;est pas correctement lue par Pandas, nous devons donc résoudre celà pour pouvoir visualiser les séries temporelles.\n# Commençons par nommer la colonnes base = base.rename({\u0026quot;Unnamed: 2\u0026quot;:\u0026quot;Année\u0026quot;}, axis= 1)  base[\u0026quot;Année\u0026quot;].head(3)  0 1963-1 1 1963-2 2 1963-3 Name: Année, dtype: object  base[\u0026quot;Année\u0026quot;].tail(3)  197 2012-T2 198 2012-T3 199 2012-T4 Name: Année, dtype: object  import warnings warnings.filterwarnings(\u0026quot;ignore\u0026quot;)  # Générons une série temporelle pd.date_range(start = \u0026quot;1963/01\u0026quot;, end = \u0026quot;2012/12\u0026quot;, freq= \u0026quot;3 m\u0026quot;)  DatetimeIndex(['1963-01-31', '1963-04-30', '1963-07-31', '1963-10-31', '1964-01-31', '1964-04-30', '1964-07-31', '1964-10-31', '1965-01-31', '1965-04-30', ... '2010-07-31', '2010-10-31', '2011-01-31', '2011-04-30', '2011-07-31', '2011-10-31', '2012-01-31', '2012-04-30', '2012-07-31', '2012-10-31'], dtype='datetime64[ns]', length=200, freq='3M')  # On peut remplacer l'année par cette série base[\u0026quot;Année\u0026quot;] = pd.date_range(start = \u0026quot;1963/01\u0026quot;, end = \u0026quot;2012/12\u0026quot;, freq= \u0026quot;3 m\u0026quot;)  base.head(3)   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    OBS UN Année CPC DAT DLT DP DSPS DUC GPG ... TDE TE TINFL TPRES TR TSP W XPX YPY TEF     0 1 1 1963-01-31 274.252399 1859.226699 40.0 14.999408 4.575718 82.798301 103.402718 ... 25.207117 19.547539 5.160079 32.019116 6.465074 -0.142149 0.759338 33.289599 476.046181 8.815188   1 2 1 1963-04-30 279.979818 1876.450261 40.0 16.075849 1.588363 84.328672 105.287914 ... 25.919505 18.990496 4.982553 32.541230 6.482865 0.302659 0.767427 33.197693 486.892013 7.582511   2 3 1 1963-07-31 284.897175 1887.905376 40.0 16.901766 -0.326363 85.182034 107.088900 ... 26.345618 18.640453 4.705747 32.955774 6.457475 0.616460 0.776522 33.452026 496.462215 6.721496    3 rows × 39 columns\n Nous pouvons maintenant enlever les colonnes OBS, et UN et choisir la colonne Année comme l\u0026rsquo;index de notre base.\nbase.drop(columns= [\u0026quot;OBS\u0026quot;, \u0026quot;UN\u0026quot;], inplace= True)  # Définissons Année comme indice base.set_index(\u0026quot;Année\u0026quot;, inplace= True)  Distribution des colonnes de la base de donnée Nous allons maintenant essayer de regarder comment sont distribuées les colonnes de notre base données. Et regarder comment elles sont corrélées entre elles.\nLa librairie Seaborn est très puissante pour l\u0026rsquo;analyse exploratoire des données car elle ajoute à matplotlib des types de graphiques inédits.\nbase.columns  Index(['CPC', 'DAT', 'DLT', 'DP', 'DSPS', 'DUC', 'GPG', 'H', 'INVP', 'K', 'L', 'MPM', 'PAP', 'PC', 'PDI', 'PG', 'PINV', 'PM', 'POP', 'PX', 'PY', 'RD', 'SP', 'TCHO', 'TCHO.1', 'TCOT', 'TDE', 'TE', 'TINFL', 'TPRES', 'TR', 'TSP', 'W', 'XPX', 'YPY', 'TEF'], dtype='object')  Etant donné qu\u0026rsquo;il y a plus de 30 variables dans la base de donnée, nous n\u0026rsquo;allons représenter que les 7 premières dans un pairplot (le principe est de représenter des combinaisons de deux variables).\nsns.pairplot(base.iloc[:, 1:7], diag_kind= \u0026quot;kde\u0026quot;)  \u0026lt;seaborn.axisgrid.PairGrid at 0x1e9cd587d30\u0026gt;  Mesure de la correlation entre les variables #base.corr()  plt.figure(figsize = (14,8)) sns.heatmap(base.corr(), cmap= \u0026quot;viridis\u0026quot;)  \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0x1e9d04f3da0\u0026gt;  Ce graphique est une représentation de la matrice des correlations. EN diagonale la correlation vaut 1 car la variable corrèle avec elle-même.\nD\u0026rsquo;une vue globale, ce qu\u0026rsquo;on remarque c\u0026rsquo;est que la plupart des variables sont corrélées entre elles.\nSi on veut facilement répérer visuellement quelles variables sont corrélées entre elles, on peut utiliser l\u0026rsquo;algorithme de classification hiérarchique pour visualiser.\nsns.clustermap(base.corr(), cmap = \u0026quot;viridis\u0026quot;)  \u0026lt;seaborn.matrix.ClusterGrid at 0x1e9d1a33c18\u0026gt;  Plus la couleur tend vers jaune, plus forte est la correlation. on remarque dans la partie gauche du graphique un fort regroupement des variables.\nOn remarque que le revenu disponible (RD) corrèle très fortement avec plusieurs variables dont la population totale, le prix de la production, le prix de la consommation. Ces correlations ne traduisent en rien une causalité, elles signifient seulement que ces variables évoluent dans le même sens.\nVisualisation des séries temporelles Après avoir transformée notre colonne année en indice, nous pouvons facilement représenter la série temporelle des variables.\nIntéressons-nous, pour commencer à l\u0026rsquo;évolution du taux de chômage en France depuis les années 1960\nbase[\u0026quot;TCHO\u0026quot;].plot() plt.title(\u0026quot;Taux de chômage en France de 1960 à 2012\u0026quot;) plt.ylabel(\u0026quot;Taux de chômage\u0026quot;)  Text(0, 0.5, 'Taux de chômage')  Il semble bien que depuis 1960, le taux de chômage en France ne cesse de croître. On observe une baisse nette vers les années 2008 avant que le taux de chômage ne reprenne de croître.\nRegardons maintenant l\u0026rsquo;évolution du travail et du capital sur la même période.\nbase[[\u0026quot;K\u0026quot;, \u0026quot;L\u0026quot;]].plot() plt.title(\u0026quot;Evolution du travail (L) et du capital (K) entre 1960-2012\u0026quot;)  Text(0.5, 1.0, 'Evolution du travail (L) et du capital (K) entre 1960-2012')  Essayons de tester la courbe de Phillips.\nPhillips était un économiste keynésien du XXè siècle, il est connu pour sa courbe qui prédisait que le taux d\u0026rsquo;inflation corrèlait négativement avec le taux de chômage. Pour lui donc, plus il y a de l\u0026rsquo;inflation, moins il y a du chômage. Essayons donc de voir l\u0026rsquo;évolution de ces indicateurs dans le temps et représentons un nuage de point pour voir leur correlation\nbase[[\u0026quot;TCHO\u0026quot;, \u0026quot;TINFL\u0026quot;]].plot() plt.title(\u0026quot; Evolution des taux d'inflation (TINFL) et du taux de chômage (TCHO)\u0026quot;)  Text(0.5, 1.0, \u0026quot; Evolution des taux d'inflation (TINFL) et du taux de chômage (TCHO)\u0026quot;)  On observe que sur cette période les taux d\u0026rsquo;inflation et de chômage fluctuent fortement. Ils se coupent deux fois, une fois première vers les années 65 à un moment où les dux taux sont très bas et une deuxième vers les années 1980.\n# Correlation sns.scatterplot(x= \u0026quot;TCHO\u0026quot;, y = \u0026quot;TINFL\u0026quot;, data = base) plt.title(\u0026quot;Nuage de points entre taux de chômage et taux d'inflation\u0026quot;) plt.xlabel(\u0026quot;Taux de chômage en %\u0026quot;) plt.ylabel(\u0026quot;Taux d'inflation en %\u0026quot;)  Text(0, 0.5, \u0026quot;Taux d'inflation en %\u0026quot;)  Il ne smble pas y avoir une relation évidente entre ces deux variables. Les deux n\u0026rsquo;évoluent ni dans le même sens ni dans le sens inverse.\nRegardons les densités de ces deux variables\nsns.kdeplot(base[\u0026quot;TCHO\u0026quot;]) sns.kdeplot(base[\u0026quot;TINFL\u0026quot;]) plt.title(\u0026quot;Densité des taux de chômage et taux d'inflation\u0026quot;)  Text(0.5, 1.0, \u0026quot;Densité des taux de chômage et taux d'inflation\u0026quot;)  Le taux de chômage a une répartition bimodale tandis que le taux d\u0026rsquo;inflation est unimodal.\nC\u0026rsquo;est quoi le lien entre le taux de chômage et la population active?\nsns.scatterplot(x = \u0026quot;TCHO\u0026quot;, y = \u0026quot;PAP\u0026quot;, data = base) plt.title(\u0026quot;Taux de chômage vs. Population active\u0026quot;) plt.xlabel(\u0026quot;Taux de chômage\u0026quot;) plt.ylabel(\u0026quot;Population active\u0026quot;)  Text(0, 0.5, 'Population active')  base[[\u0026quot;POP\u0026quot;, \u0026quot;PAP\u0026quot;]].plot() plt.title(\u0026quot;Evolution de la population active (PAP) et de la population totale (POP)\u0026quot;)  Text(0.5, 1.0, 'Evolution de la population active (PAP) et de la population totale (POP)')  Comment évolue la dette publique?\nbase[\u0026quot;DP\u0026quot;].plot() plt.title(\u0026quot;Evolution de la dette publique en Milliard d'€\u0026quot;) plt.show()  la dette publique a une augmentation exponentielle avec le temps. Une très légère baisse s\u0026rsquo;observe vers 2008 mais le cours de l\u0026rsquo;augmentation reprend toute juste après\nbase[\u0026quot;TPRES\u0026quot;].plot() plt.title(\u0026quot;Taux de prélèvement obligatoire en % \u0026quot;)  Text(0.5, 1.0, 'Taux de prélèvement obligatoire en % ')  La France est l\u0026rsquo;un des pays qui ont les taux de prélèvements obligatoires très élevés au monde. SUr ce graphique, on observe que ce n\u0026rsquo;était pas le cas au début des années 1960.\nbase[[\u0026quot;PY\u0026quot;, \u0026quot;PC\u0026quot;]].plot() plt.title(\u0026quot;Evolution des prix de la production (PY) et prix de la consommation\u0026quot;) plt.show()  Ces deux sont évoluent très étroitement.\nEstimation d\u0026rsquo;une fonction de la consommation keynésienne Première estimation : modèle double linéaire La fonction de consommation keynésienne se présente comme suit: \\begin{equation*} C = C_0 + _c * y^d \\end{equation*}\nLa consommation C est égale à la comsommation incompréssible (le niveau de consommation quand le revenu est égal à 0) + la propension marginale à consommer multipliée par le revenu disponible.\nNous allons estimer cette fonction grâce à notre base de donnée\nimport numpy as np  import statsmodels.api as sm  La notation matricielle d\u0026rsquo;une équation économétrique se présente comme suit :\n\\begin{equation} y = cste + X + \\epsilon\n\\end{equation}\n# Choisissons y notre variable endogène: celle qu'on veut expliquer. Ici c'est la consommation y = base[\u0026quot;CPC\u0026quot;]  # Trouvons X la matrice des variables exogènes; dans ce cas on ne prend que le revenu disponible X = base[\u0026quot;RD\u0026quot;] # Ajoutons une constante à la base X = sm.add_constant(X)  # Estimons premiere_estimation = sm.OLS(endog= y, exog= X).fit()  premiere_estimation.summary()  OLS Regression Results  Dep. Variable: CPC  R-squared:   0.992   Model: OLS  Adj. R-squared:   0.992   Method: Least Squares  F-statistic:  2.485e+04   Date: Tue, 19 Feb 2019  Prob (F-statistic): 4.49e-210   Time: 21:23:43  Log-Likelihood:   -877.87   No. Observations:  200  AIC:   1760.   Df Residuals:  198  BIC:   1766.   Df Model:  1    \n  Covariance Type: nonrobust    \n    coef std err t P\u0026gt;|t| [0.025 0.975]\n  const  -94.0566  5.029  -18.702  0.000  -103.974  -84.139   RD  1.0772  0.007  157.627  0.000  1.064  1.091    Omnibus:  5.737  Durbin-Watson:   0.034   Prob(Omnibus):  0.057  Jarque-Bera (JB):   5.557   Skew: -0.321  Prob(JB):   0.0621   Kurtosis:  3.504  Cond. No.  2.67e+03  Warnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 2.67e+03. This might indicate that there arestrong multicollinearity or other numerical problems.\nA l\u0026rsquo;estimation de ce modèle on retrouve des un résultat surprenant. La constante qui est supposée être notre consommation incompressible est négtive, économétriquement ces résultats sont bons (R² proche de 1, p-values des deux coefficients très faibles) mais on observe bien que la théorie économique sous-jacente n\u0026rsquo;est pas vérifiée. De plus la valeur du Durbin-Watson est loin de 2, ce qui signifie qu\u0026rsquo;il existe une autocorrelation des résidus.\nIl nous faut donc transformer les variables.\nTraitement des erreurs from scipy import stats  premiere_estimation.resid.plot() plt.title(\u0026quot;Erreurs d'estimation\u0026quot;)  Text(0.5, 1.0, \u0026quot;Erreurs d'estimation\u0026quot;)  stats.shapiro(premiere_estimation.resid)  (0.9824138879776001, 0.013233800418674946)  from matplotlib.ticker import NullFormatter plt.style.use(\u0026quot;seaborn\u0026quot;) plt.figure(1, dpi = 120) # Residuals vs. Predicted values plt.subplot(221) plt.scatter(premiere_estimation.fittedvalues, base[\u0026quot;CPC\u0026quot;]) plt.title(\u0026quot;Valeurs prédites vs. Valeurs observées\u0026quot;) plt.xlabel(\u0026quot;Valeurs prédites\u0026quot;) plt.ylabel(\u0026quot;Valeurs observées\u0026quot;) plt.grid(True) # Predicted values vs. Residuals plt.subplot(222) plt.scatter(premiere_estimation.fittedvalues, premiere_estimation.resid) plt.title(\u0026quot;Valeurs prédites vs. Résidus\u0026quot;) plt.xlabel(\u0026quot;Valeurs prédites\u0026quot;) plt.ylabel(\u0026quot;Résidus\u0026quot;) plt.grid(True) # Residuals plt.subplot(223) sns.distplot(premiere_estimation.resid) plt.grid(True) plt.gca().yaxis.set_minor_formatter(NullFormatter())  plt.style.use(\u0026quot;fivethirtyeight\u0026quot;) sm.qqplot(premiere_estimation.resid)  sns.kdeplot(premiere_estimation.resid)  \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0x1e9d8e7aac8\u0026gt;  Deuxième estimation : modèle semi-linéaire Dans ce modèle, nous allons transformer la variable exogène pour la linéariser\nX = np.log(X)  # Estimons seconde_estimation = sm.OLS(endog= y, exog= X).fit()  seconde_estimation.summary()  OLS Regression Results  Dep. Variable: CPC  R-squared:   0.929   Model: OLS  Adj. R-squared:   0.928   Method: Least Squares  F-statistic:   2590.   Date: Tue, 19 Feb 2019  Prob (F-statistic): 4.78e-116   Time: 21:23:46  Log-Likelihood:   -1330.9   No. Observations:  200  AIC:   2664.   Df Residuals:  199  BIC:   2667.   Df Model:  1    \n  Covariance Type: nonrobust    \n    coef std err t P\u0026gt;|t| [0.025 0.975]\n  const  0  0  nan  nan  0  0   RD  103.8591  2.041  50.893  0.000  99.835  107.883    Omnibus: 63.669  Durbin-Watson:   0.000   Prob(Omnibus):  0.000  Jarque-Bera (JB):   10.656   Skew:  0.011  Prob(JB):   0.00485   Kurtosis:  1.869  Cond. No.   inf  Warnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The smallest eigenvalue is 0. This might indicate that there arestrong multicollinearity problems or that the design matrix is singular.\nCe modèle est encore plus pire que le dernier que nous avons estimé. Nous allons ajouter effectuer des transformations puis réestimer.\nModèle double log avec variable de retard Dans un modèle avec une variable de retard on décale d\u0026rsquo;un rang notre variable endogène et on l\u0026rsquo;introduit dans le modèle comme variable explicative. \\begin{equation} y_t =y_t-_1 + X + epsilon \\end{equation}\nnouvelle_base = base[[\u0026quot;CPC\u0026quot;, \u0026quot;RD\u0026quot;]] nouvelle_base[\u0026quot;log_rd\u0026quot;] = np.log(nouvelle_base[\u0026quot;RD\u0026quot;]) nouvelle_base[\u0026quot;log_y\u0026quot;] = np.log(nouvelle_base[\u0026quot;CPC\u0026quot;]) nouvelle_base[\u0026quot;y_retardée\u0026quot;] = np.roll(nouvelle_base[\u0026quot;log_y\u0026quot;], shift= -1) nouvelle_base.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    CPC RD log_rd log_y y_retardée   Année          1963-01-31 274.252399 323.844394 5.780263 5.614049 5.634718   1963-04-30 279.979818 328.496646 5.794527 5.634718 5.652128   1963-07-31 284.897175 332.822369 5.807609 5.652128 5.666878   1963-10-31 289.130436 336.893221 5.819766 5.666878 5.679329   1964-01-31 292.752877 340.720822 5.831063 5.679329 5.690124     y = nouvelle_base[\u0026quot;log_y\u0026quot;] X = nouvelle_base[[\u0026quot;y_retardée\u0026quot;, \u0026quot;log_rd\u0026quot;]]  # Ajoutons la constante X = sm.add_constant(X)  # Estimons le modèle troisieme_modele = sm.OLS(endog= y, exog = X).fit()  troisieme_modele.summary()  OLS Regression Results  Dep. Variable: log_y  R-squared:   0.992   Model: OLS  Adj. R-squared:   0.991   Method: Least Squares  F-statistic:  1.153e+04   Date: Tue, 19 Feb 2019  Prob (F-statistic): 8.04e-205   Time: 21:23:46  Log-Likelihood:   394.89   No. Observations:  200  AIC:   -783.8   Df Residuals:  197  BIC:   -773.9   Df Model:  2    \n  Covariance Type: nonrobust    \n    coef std err t P\u0026gt;|t| [0.025 0.975]\n  const  -1.0456  0.055  -19.005  0.000  -1.154  -0.937   y_retardée  0.1077  0.025  4.300  0.000  0.058  0.157   log_rd  1.0428  0.029  35.601  0.000  0.985  1.101    Omnibus: 22.033  Durbin-Watson:   0.119   Prob(Omnibus):  0.000  Jarque-Bera (JB):   81.919   Skew:  0.255  Prob(JB):  1.63e-18   Kurtosis:  6.093  Cond. No.   233.  Warnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\ntroisieme_modele.resid.plot()  \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0x1e9d8e52c88\u0026gt;  Les erreurs ne sont toujours pas bruit blanc, pour améliorer la capacité du modèle à expliquer la réalité il nous faudra ajouter d\u0026rsquo;autres explicatives. Ce modèle était simpliste.\nAvant de finir j\u0026rsquo;aimerais vous montrer la règle d\u0026rsquo;interprétation d\u0026rsquo;un tel modèle. Etant donnée qu\u0026rsquo;on a regressé un modèle log-log, les coefficients s\u0026rsquo;interprêront comme une élasticité.\nDans ce modèle en l\u0026rsquo;occurence, l\u0026rsquo;augmentation du revenu disponible d\u0026rsquo;un 1% entraîne l\u0026rsquo;augmentation de la consommation de 1.04%\n","date":1555632000,"expirydate":-62135596800,"kind":"page","lang":"fr","lastmod":1555632000,"objectID":"90fe62cb536f5e3dcd7809eee7cbaea1","permalink":"https://agailloty.rbind.io/fr/post/econometrics-python/","publishdate":"2019-04-19T00:00:00Z","relpermalink":"/fr/post/econometrics-python/","section":"post","summary":"Ce projet implémente en Python la trame générale d'une étude économétrique. Il s'agit d'estimer la relation linéaire entre le revenu et la consommation","tags":["Python","pandas","statsmodels"],"title":"Estimer la relation linéaire entre le revenu et la consommation (python)","type":"post"},{"authors":["Axel-Cleris Gailloty"],"categories":["R","ACP"],"content":" Table des matières HAHAHUGOSHORTCODE-TOC0-HBHB\nL\u0026rsquo;Analyse en Composante Principale (ACP) Intuition L\u0026rsquo;un des clés essentiels au succès d\u0026rsquo;un projet de data science est de comprendre le problème qu\u0026rsquo;on veut bien résoudre. La plupart des data scientists affirment que plus de 70% du temps qu\u0026rsquo;ils consacrent à la résolution d\u0026rsquo;un problème donné est consacré au nettoyage de la base de donnée et à la compréhension du problème. Donc avant même d\u0026rsquo;appliquer n\u0026rsquo;importe quel algorithme de Machine Learning il faut comprendre pourquoi on fait ce choix et quels sont les bénéfices pour la résolution du problème.\nL\u0026rsquo;analyse en composante principale est une l\u0026rsquo;un des outils qui font partie de l\u0026rsquo;arsenal des professionnels en science des données car il permet (1) de résumer et (2) réduire la dimension d\u0026rsquo;un jeu de données. Que faites-vous si vous avez un jeu de données contenant 10000 individus et 30 variables quantitatives? On peut essayer de calculer manuellement la moyenne, les écarts-types pour essayer de comprendre le jeu de données mais cela est long est fastidieux et on ne pourrait pas vraiment résumer l\u0026rsquo;information. C\u0026rsquo;est alors que vient l\u0026rsquo;ACP.\nDans cet article, je vous épargnes les détails techniques de l\u0026rsquo;ACP que vous pouvez trouver partout sur Internet. A ce sujet je vous conseille fortement les vidéos de François Husson qui sont sur Youtube et qui sont d\u0026rsquo;une grande qualité. Lui et l\u0026rsquo;équipe d\u0026rsquo;AgroCampus Ouest sont les auteurs du package (et même de la base de donnée que j\u0026rsquo;ai eu en suivant son MOOC sur FUN-MOOC) que je vais utiliser pour la réalisation de ce projet.\nLes scripts ainsi que le notebook sont disponibles dans ce répertoire GitHub. Chargement de la base Je travaillerai avec le langage R et plus précisément avec le package FactoMiner. Comme j\u0026rsquo;aime bien faire de beaux graphiques, j\u0026rsquo;utiliserai les packages ggplot2 et autoplot pour représenter les graphiques d\u0026rsquo;interprétations. Ce projet est destiné à offrir un pipeline cohérent pour réaliser vos études statistiques multivariées.\nlibrary(FactoMineR) library(ggplot2) library(ggfortify)  La base de donnée sur laquelle je vais travailler concerne la fertilité moyenne des femmes dans 39 pays européens par catégorie d\u0026rsquo;âge: les 15-19 ans, 20-24 ans \u0026hellip;\nfertility \u0026lt;- read.csv(\u0026quot;data_PCA_Fertility.csv\u0026quot;, sep= ';', row.names=1) #  # Regardons comment se présente la base head(fertility)   X15.19X20.24X25.29X30.34X35.39X40.et..Area  Albania20.8 107.1126.7 72.324.4 5.5 South Austria 8.6  44.5 89.2 94.646.8 9.6 West  Belarus21.4  90.4106.7 67.626.0 4.4 East  Belgium 9.0  52.6127.0116.648.3 9.6 West  Bosnia and Herzegovina11.0  52.5 91.8 69.725.7 4.7 South Bulgaria42.1  71.9 90.9 67.127.2 4.7 East    dim(fertility)  39 7  Nous avons 39 individus et 7 variables.\nAnalyse exploratoire Statistiques descriptives Cette partie est pour explorer la base de données afin de comprendre de quoi il est question, de la nettoyer si possible et la visualiser.\nsummarytools::descr(fertility, transpose = T)   MeanStd.DevMinQ1MedianQ3MaxMADIQRCVSkewnessSE.SkewnessKurtosisN.ValidPct.Valid  X15.1914.333333  8.913365  3.5  8.0 12.0  19.2  42.1  8.30256 11.05 0.6218627 1.19413200 0.3782198  1.1794559139 100  X20.2455.348718 18.676243 29.2 41.3 52.5  68.4 107.1 19.12554 25.30 0.3374286 0.81770434 0.3782198  0.0127880439 100  X25.2997.271795 17.018395 57.6 85.2 93.8 108.4 132.1 19.12554 22.15 0.1749571 0.02981643 0.3782198 -0.6104296939 100  X30.3493.843590 22.795196 46.1 76.5 91.4 112.3 136.7 28.02114 34.75 0.2429063 0.17612427 0.3782198 -0.9143701939 100  X35.3945.197436 16.075750 19.9 32.7 43.4  57.3  97.5 18.38424 24.00 0.3556784 0.75110236 0.3782198  0.7835839839 100  X40.et.. 9.533333  4.191428  3.8  6.6  9.5  12.5  23.8  4.29954  5.45 0.4396603 0.99832021 0.3782198  1.3243700839 100    La fonction descr du package summarytools nous affiche des élements utiles concernant notre base de données. Ce qu\u0026rsquo;on remarque dans un premier c\u0026rsquo;est les deux dernières colonnes qui nous indique le nombre de données valides. Heureusement pour nous aucune donnée n\u0026rsquo;est manquante, cela nous épargne du temps.\nTout à gauche les 5 premières colonnes nous présentent les statistiques descriptives du jeu de données. On observe par exemple que la moyenne de la fertilité des femmes de plus de 40 ans est la plus faible, suivie de celle des 15-19 ans. L\u0026rsquo;écart-type de ces moyenne est reltivement élevé pour les femmes de 30-34, ce qui explique qu\u0026rsquo;il y a une assez forte variance entre les pays.\nDonnées groupées Groupons maintenant ces données par région (Area) pour voir les moyennes des variables mais par régions.\nlibrary(tidyverse)  group_by(fertility, Area) %\u0026gt;% summarise_all(mean)   AreaX15.19X20.24X25.29X30.34X35.39X40.et..  East 24.76000068.12000  90.9400  70.8400 30.18000  5.76000  North 10.71000053.11000 107.3900 111.6700 58.68000 13.05000  South 12.85000053.76667  92.3750  86.9500 41.14167  8.87500  West  7.15714343.01429 100.2571 113.0571 54.34286 11.02857    Ici nous avons les moyennes par régions. L\u0026rsquo;indication qu\u0026rsquo;on avait eu de la variance des 30-34 se confirme ici car on observe que la moyenne des fécondités varie fortement par région. Les 15-19 ont une plus grande fertilité en Europe de l\u0026rsquo;Est que dans les autres régions.\nVisualisation options(repr.plot.res = 300, repr.plot.width = 6, repr.plot.height = 3)  Fertilité des 15-19 par pays row.names(fertility)  'Albania' 'Austria' 'Belarus' 'Belgium' 'Bosnia and Herzegovina' 'Bulgaria' 'Croatia' 'Denmark' 'Estonia' 'Finland' 'France' 'Germany' 'Greece' 'Hungary' 'Iceland' 'Ireland' 'Italy' 'Latvia' 'Lithuania' 'luxembourg' 'Macedonia' 'Malta' 'Moldova' 'Montenegro' 'Netherlands' 'Norway' 'Poland' 'Portugal' 'Rep. Czech' 'Romania' 'Russia' 'Serbia' 'Slovakia' 'Slovenia' 'Spain' 'Sweden' 'Swiss' 'UK' 'Ukraine'  plot_fertility \u0026lt;- function(data, x=row.names(fertility), y, size_text = 3, ...){ ggplot(data, aes(x = x)) + geom_text(aes(y=y, label = x), size = size_text) + theme_classic() + theme(axis.text.x = element_blank()) + xlab(\u0026quot; \u0026quot;) }  plot_fertility(fertility, y = fertility$X15.19, size_text = 2.4) + ggtitle(\u0026quot;Fertilité des 15-19 ans\u0026quot;) + ylab(\u0026quot;Fertilité\u0026quot;)  plot_fertility(fertility, y = fertility$X20.24, size_text = 2.4) + ggtitle(\u0026quot;Fertilité des 20-24 ans\u0026quot;) + ylab(\u0026quot;Fertilité\u0026quot;)  plot_fertility(fertility, y = fertility$X25.29, size_text = 2.4) + ggtitle(\u0026quot;Fertilité des 25-29 ans\u0026quot;) + ylab(\u0026quot;Fertilité\u0026quot;)  La France arrive en tête de la fertilité moyenne des 25-29 ans.\nCorrélations library(ggcorrplot)  Warning message: \u0026quot;package 'ggcorrplot' was built under R version 3.5.2\u0026quot;  ggcorrplot(corr = cor(fertility[,-7]), hc.order = TRUE, type = \u0026quot;lower\u0026quot;, lab = TRUE)  Ce graphique nous donne les corrélations entre les variables quantitatives du jeu de donnée. Par exemple la fertilité des 35-39 corrèle très fortement avec la fertilité des 40 ans et plus.\nL\u0026rsquo;Analyse en composante principale Comme on le voit au travers de ces graphiques, certaines variables différencient bien les pays, d\u0026rsquo;autres pas vraiment. On ne peut juste avoir qu\u0026rsquo;un aperçu mais pas un résumé complet qui nous permet de comprendre les facteurs de différence entre les pays. l\u0026rsquo;ACP nous aidera à mieux représenter les individus\nConstruction acp \u0026lt;- PCA(X = fertility, scale.unit = T, quali.sup = 7)  On vient de créer l\u0026rsquo;objet acp qui contient les résultats de l\u0026rsquo;analyse. Les arguments entrés dans la fonction PCA : X représente le jeu de donnée, scale.unit = T car on veut mettre à l\u0026rsquo;échelle toutes les variables et quali.sup = 7 c\u0026rsquo;est notre variable qualitative supplémentaire (7e colonne du jeu de données).\n# Voyons ce que contient l'objet acp names(acp)  'eig' 'var' 'ind' 'svd' 'quali.sup' 'call'  Nous allons d\u0026rsquo;abord voir un résumé.\nsummary(acp, ncp = 2)  Call: PCA(X = fertility, scale.unit = T, quali.sup = 7) Eigenvalues Dim.1 Dim.2 Dim.3 Dim.4 Dim.5 Dim.6 Variance 3.702 1.360 0.677 0.206 0.046 0.009 % of var. 61.704 22.665 11.287 3.427 0.772 0.145 Cumulative % of var. 61.704 84.369 95.656 99.084 99.855 100.000 Individuals (the 10 first) Dist Dim.1 ctr cos2 Dim.2 ctr cos2 Albania | 3.883 | -2.879 5.742 0.550 | 2.435 11.182 0.393 | Austria | 1.007 | 0.567 0.222 0.317 | -0.647 0.789 0.413 | Belarus | 2.990 | -2.762 5.283 0.853 | 0.964 1.751 0.104 | Belgium | 2.141 | 0.937 0.608 0.192 | 1.730 5.643 0.653 | Bosnia and Herzegovina | 2.073 | -1.425 1.406 0.472 | -0.670 0.846 0.104 | Bulgaria | 3.870 | -3.359 7.816 0.754 | -0.338 0.216 0.008 | Croatia | 0.651 | -0.244 0.041 0.140 | 0.043 0.004 0.004 | Denmark | 2.352 | 1.989 2.740 0.715 | 0.774 1.131 0.108 | Estonia | 0.537 | -0.086 0.005 0.026 | 0.086 0.014 0.026 | Finland | 1.729 | 1.569 1.704 0.823 | 0.697 0.916 0.163 | Variables Dim.1 ctr cos2 Dim.2 ctr cos2 X15.19 | -0.831 18.668 0.691 | -0.024 0.042 0.001 | X20.24 | -0.728 14.319 0.530 | 0.533 20.899 0.284 | X25.29 | 0.041 0.046 0.002 | 0.979 70.413 0.958 | X30.34 | 0.908 22.280 0.825 | 0.342 8.591 0.117 | X35.39 | 0.942 23.981 0.888 | 0.026 0.049 0.001 | X40.et.. | 0.876 20.705 0.767 | -0.008 0.005 0.000 | Supplementary categories Dist Dim.1 cos2 v.test Dim.2 cos2 v.test East | 2.191 | -2.143 0.957 -4.032 | -0.338 0.024 -1.050 | North | 1.618 | 1.414 0.763 2.660 | 0.703 0.189 2.184 | South | 0.553 | -0.243 0.194 -0.519 | -0.375 0.460 -1.321 | West | 1.529 | 1.459 0.911 2.186 | 0.121 0.006 0.298 |  Ici j\u0026rsquo;ai choisi de n\u0026rsquo;afficher que les détails des deux premières composantes principales (ncp =2).\nLes eignevalues représentent les valeurs propres. La première composante résume à elle seule près de 62% de la variance des individus, ce qui fait si on ajoute à ce pourcentage de variance, le pourcentage de variance de la deuxième composante nous résumons 84% de l\u0026rsquo;information. C\u0026rsquo;est un excellent score. Donc au lieu de 6 variables explicatives, seules 2 variables synthétiques suffisent pour comprendre la variabilité des individus. Nous verrons comment commenter ces variables.\nGraphique des variables Représentons maintenant le graphique des variables sur les deux prémières composantes\noptions(repr.plot.res = 150, repr.plot.width = 5, repr.plot.height = 5) plot.PCA(acp, choix = \u0026quot;var\u0026quot;)  Comment lire ce graphique ? Les deux axes de ce graphique représentent les coefficients de corrélations qui vont de -1 à 1. Les flèches indiquent à la fois la corrélation de chacune des variables à l\u0026rsquo;axe donnée et la corrélation des variables entre elles. Comme nous l\u0026rsquo;avions déja anticipé dans la partie corrélation on voit que la fertilité des femmes 35-39 ans corrèle fortement avec celle des femmes de plus de 40 ans. Et il se trouve également que ce sont ces deux variables + la variable des 30-34 qui sont les plus corrélées positivement avec l\u0026rsquo;axe 1. La variable 15-19 quant à elle est très négativement corrélée avec cette prémière axe.\nLa variable X25-29 corrèle très fortement avec l\u0026rsquo;axe 2, les autres X20-24 et X30-34 corrèlent aussi positivement mais de manière modérée.\nGraphique des individus options(repr.plot.width = 7) plot.PCA(acp, choix= \u0026quot;ind\u0026quot;)  Ce graphique représente la distance des pays du barycentre. Un pays situé au centre de ce graphique a tendance à avoir des valeurs moyennes pour toutes les variables, inversement les pays qui s\u0026rsquo;éloignent du barycentre sont ont des valeurs éloignés de la moyenne et enfin la distance entre les pays traduit leur différence ou similitude. Pour voir un exemple, comparons les valeurs que prennent l\u0026rsquo;Espagne, l\u0026rsquo;Albanie et la Croatie\nfertility[c(\u0026quot;Spain\u0026quot;, \u0026quot;Albania\u0026quot;, \u0026quot;Croatia\u0026quot;),]   X15.19X20.24X25.29X30.34X35.39X40.et..Area  Spain 8.9  29.2 57.691.2 62.4 14.9 South Albania20.8 107.1126.772.3 24.4  5.5 South Croatia11.4  54.6 99.191.4 39.9  7.8 South   # Moyennes des colonnes t(summarytools::descr(fertility)[\u0026quot;Mean\u0026quot;,])   X15.19X20.24X25.29X30.34X35.39X40.et..  14.3333355.3487297.2717993.8435945.197449.533333   On voit ici que tout semble opposer l\u0026rsquo;Espagne et l\u0026rsquo;Albanie. L\u0026rsquo;Espagne a tendance à avoir des valeurs faibles pour les variables X15.19, X20.24 et X25.29 là où l\u0026rsquo;Albanie enregistre de forte valeurs, et similairement l\u0026rsquo;Espagne semble avoir des valeurs plus élevées pour X30.34, X35-39, X40.\nLa Croatie qui se situe au près du barycentre a des valeurs proches de la moyenne des colonnes.\nComment comprendre le graphique des individus? L\u0026rsquo;interprétation du graphique des individus se fait en ayant en tête le cercle des corrélations (graphique des variables). Sur l\u0026rsquo;axe 1 les pays situés à droite sont les pays dans lesquels la fertilité des femmes de plus de 30 ans est la plus élévée et inversement pour les pays situés à gauche.\nInfluence des variables et des individus Influence des variables Dans cette section, il est question d\u0026rsquo;examiner les coordonnées des individus et des variables pour voir celles qui contribuent à la définition des axes, autrement y a-t-il des variables ou des individus sur-représentées dans la précédente analyse?\nacp$var$contrib[, c(\u0026quot;Dim.1\u0026quot;, \u0026quot;Dim.2\u0026quot;)]   Dim.1Dim.2  X15.1918.66806198  0.042332761 X20.2414.31907798 20.899108658 X25.29 0.04641997 70.412919869 X30.3422.28022806  8.591130345 X35.3923.98076041  0.049487266 X40.et..20.70545160  0.005021103   Ce tableau nous montre la contribution des variables dans la définition des 2 axes. L\u0026rsquo;axe 1 est bien équilibrée car la contribution des variables est bien proportionnée, aucune variable n\u0026rsquo;est sur-représentée. Ce constat n\u0026rsquo;est pas valide pour la dimension 2 car la variable X25-29 semble sur-représentée avec 70% de la contribution à la définition de l\u0026rsquo;axe 2.\nInfluence des individus Quelles sont les individus qui influencent les plus les résultats de l\u0026rsquo;ACP? Nous chercherons pour axe les pays qui ont les plus d\u0026rsquo;impacts dans la construction de chacune des axes.\nPremière axe individus \u0026lt;- acp$ind$contrib[, c(\u0026quot;Dim.1\u0026quot;, \u0026quot;Dim.2\u0026quot;)] %\u0026gt;% data.frame  options(scipen = 99) #Les décimales après la virgule  individus[order(individus$Dim.1, decreasing = T),][\u0026quot;Dim.1\u0026quot;] %\u0026gt;% head()   Dim.1  Ireland11.828437 Moldova 8.489856 Bulgaria 7.816444 Ukraine 7.277283 Romania 6.452351 Albania 5.742210   sum(individus[order(individus$Dim.1, decreasing = T),][\u0026quot;Dim.1\u0026quot;] %\u0026gt;% head())  47.6065807477098\nCes individus sont les mieux représentés sur l\u0026rsquo;axe. A eux seuls ils contribuent à hauteur de 47% à la construction de l\u0026rsquo;axe 1.\nLa deuxième axe individus[order(individus$Dim.2, decreasing = T),][\u0026quot;Dim.2\u0026quot;] %\u0026gt;% head()   Dim.2  Spain13.110490 Albania11.182419 France 9.910815 Portugal 6.634490 Belgium 5.642806 Italy 5.438278   sum(individus[order(individus$Dim.2, decreasing = T),][\u0026quot;Dim.2\u0026quot;] %\u0026gt;% head())  51.9192973349872\nIci ces 6 individus contribuent à hauteur de 52% à la définition de l\u0026rsquo;axe 2.\nClassification ascendante hiérarchique (CAH) Nous voulons maintenant classer les individus en fonction de leurs ressemblances dans des clusters. C\u0026rsquo;est ce que nous permet de faire l\u0026rsquo;algorithme de la classification ascendante hiérarchique.\ncluster \u0026lt;- HCPC(res= acp, nb.clust = 5)  cluster$data.clust %\u0026gt;% head()   X15.19X20.24X25.29X30.34X35.39X40.et..Areaclust  Albania20.8 107.1126.7 72.324.4 5.5 South1  Austria 8.6  44.5 89.2 94.646.8 9.6 West 3  Belarus21.4  90.4106.7 67.626.0 4.4 East 1  Belgium 9.0  52.6127.0116.648.3 9.6 West 4  Bosnia and Herzegovina11.0  52.5 91.8 69.725.7 4.7 South2  Bulgaria42.1  71.9 90.9 67.127.2 4.7 East 1    Nous avons une copie de notre base de donnée\nGain d\u0026rsquo;inertie plot.HCPC(cluster, choice = \u0026quot;bar\u0026quot;)  Ce graphique nous montre le gain d\u0026rsquo;inertie associé à l\u0026rsquo;ajout d\u0026rsquo;un cluster supplémentaire. Nous observons sur ce graphique qu\u0026rsquo;au delà de 4 clusters le gain d\u0026rsquo;inertie n\u0026rsquo;est plus significatif. Essayons d\u0026rsquo;abord de voir la proportion des individus dans chaque cluster\ncluster$data.clust %\u0026gt;% group_by(clust) %\u0026gt;% summarize(nombre_individus = n())   clustnombre_individus  1  8 2 12 3  8 4 10 5  1   Nous voyons que le cluster 1 ne compte qu\u0026rsquo;un seul pays, donc le graphique précédent nous a donné une bonne indication, il faudrait réduire le nombre de clusters à 4 pour être plus efficient.\ncluster_2 \u0026lt;- HCPC(res = acp, nb.clust = 4)  plot.HCPC(cluster_2, choice = \u0026quot;bar\u0026quot;)  cluster_2$data.clust %\u0026gt;% group_by(clust) %\u0026gt;% summarize(nombre_individus = n())   clustnombre_individus  1  8 2 14 3  7 4 10   Le classement est beaucoup plus équilibré avec 4 clusters.\nEtude des cluster On se donne maintenant la tâche d\u0026rsquo;étudier les clusters afin de comprendre les différences entre entre clusters et les similitudes entre les individus regroupés ensemble.\nplot.HCPC(cluster_2, choice = \u0026quot;map\u0026quot;)  Caractéristiques des clusters Cluster 1 cluster_2$desc.var$quanti$\u0026quot;1\u0026quot;   v.testMean in categoryOverall meansd in categoryOverall sdp.value  X20.24 4.871131 84.0250 55.348718 11.855774 18.435249 0.000001109614 X15.19 4.739694 27.6500 14.333333  7.642153  8.798349 0.000002140412 X40.et..-3.412325  5.0250  9.533333  0.907951  4.137343 0.000644111719 X35.39-3.692305 26.4875 45.197436  3.608822 15.868312 0.000222231138 X30.34-3.911592 65.7375 93.843590  8.906730 22.501052 0.000091689697   Les pays du cluster 1 sont caractérisés par une fertilité des femmes de 15 à 24 supérieure aux autres pays. En moyenne la fertilité des femmes de cette tranche d\u0026rsquo;âge est de 84 alors que la moyenne générale est de 55.\nPar effet inverse la fertilité des femmes de plus de 30 ans est relativement faible que dans les autres pays. Les pays qui caractérisent ce cluster sont :\ncluster_2$desc.ind$para$\u0026quot;1\u0026quot;  Ukraine 0.689737008366458 Russia 0.826626850576947 Belarus 0.938331050874943 Macedonia 1.30786149412495 Romania 1.42674483831148  Cluster 2 cluster_2$desc.var$quanti$\u0026quot;2\u0026quot;   v.testMean in categoryOverall meansd in categoryOverall sdp.value  X35.39-1.99474938.33571 45.19744 5.78787 15.86831 0.0460703   Les pays qui sont dans ce cluster ne se distinguent des autres pays que par la fertilité des femmes des 35 à 39 relativement plus faible que la moyenne des autres. Les pays qui caractérisent ce cluster sont :\ncluster_2$desc.ind$para$\u0026quot;2\u0026quot;  Croatia 0.441189988156594 Poland 0.750448903336045 Latvia 0.864624236753571 Rep. Czech 0.920514950124321 Estonia 0.992402672777697  cluster_2$desc.var$quanti$\u0026quot;3\u0026quot;   v.testMean in categoryOverall meansd in categoryOverall sdp.value  X40.et.. 2.047435 12.471429  9.533333 2.384901  4.137343 0.0406153689 X15.19-2.154959  7.757143 14.333333 2.404078  8.798349 0.0311650310 X20.24-3.247185 34.585714 55.348718 3.771104 18.435249 0.0011655268 X25.29-3.851882 74.828571 97.271795 8.196814 16.798793 0.0001172135   Ce qui caractérise les pays présents dans ce cluster c\u0026rsquo;est leur fertilité des femmes de plus 40 plus élévée que la moyenne des autres pays. Les moyennes des fertilités des femmes de 15 à 29 ans sont faibles comparé aux autres pays. Les pays caractéristiques de ce cluster sont :\ncluster_2$desc.ind$para$\u0026quot;3\u0026quot;  Greece 0.559213141015855 Germany 0.833195757525693 luxembourg 0.914814217056602 Italy 0.99461389438098 Swiss 1.04392232825817  Cluster 4 cluster_2$desc.var$quanti$\u0026quot;4\u0026quot;   v.testMean in categoryOverall meansd in categoryOverall sdp.value  X30.34 4.928649 124.48 93.843590  7.732632 22.501052 0.0000008280034 X35.39 4.020047  62.82 45.197436 12.756159 15.868312 0.0000581864498 X40.et.. 3.453041  13.48  9.533333  3.957221  4.137343 0.0005543049357 X25.29 3.404258 113.07 97.271795 11.713159 16.798793 0.0006634407437 X15.19-2.416441  8.46 14.333333  3.943653  8.798349 0.0156730464208   Les pays qui se trouvent dans ce cluster ont une fertilité bien répartie sur toutes les catégories d\u0026rsquo;âge sauf les femmes de 15-19 qui ont une fertilité plus faible que la moyenne.\nConclusion En conclusion, l\u0026rsquo;ACP est une méthode très puissante et largement utilisée dans tous les domaines qui traitent des données. Il n\u0026rsquo;est donc pas étonnant qu\u0026rsquo;en économie on l\u0026rsquo;utilise grandement pour étudier les caractéristiques économiques des pays, les disparités etc\u0026hellip; Les variables synthétiques que produit une ACP rendent facile la compréhension et l\u0026rsquo;interprétation d\u0026rsquo;un jeu de donnée multidimensionnel\n","date":1552348800,"expirydate":-62135596800,"kind":"page","lang":"fr","lastmod":1552348800,"objectID":"2a6d63cf510995b08f7ea90c913eae01","permalink":"https://agailloty.rbind.io/fr/post/acp/","publishdate":"2019-03-12T00:00:00Z","relpermalink":"/fr/post/acp/","section":"post","summary":"Etudier le taux de fertilité des femmes dans les pays européens","tags":["R","ACP","data analysis"],"title":"Analyse en composantes principales","type":"post"},{"authors":["Axel-Cleris Gailloty"],"categories":["Python"],"content":" Lire des données financières Aujourd\u0026rsquo;hui, grâce à la performance des ordinateurs et leurs puissance de calcul qui ne cesse d\u0026rsquo;augmenter il est facile de traiter une quantité phénoménale des données brutes en très peu de temps. Le traitement et l\u0026rsquo;extraction d\u0026rsquo;informations à partir de ces données en temps réel deviennent décisifs à mesure que croissent les enjeux associés à l\u0026rsquo;utilisation d\u0026rsquo;une information. Presque toutes les grosses entreprises de la planète sont aujourd\u0026rsquo;hui côtées en bourse: que ce soit Facebook, Google, Amazon ou encore Apple pour ne citer que ces populaires là, les informations les concernant doivent être disponibles ainsi que les prédictions associées pour aider les acteurs des marchés financiers à se décider.\nDans cet article, j\u0026rsquo;aimerais vous emmener un peu à l\u0026rsquo;aventure de traitement des données en passant premièrement à leur lecture en ligne, à la visualisation et à l\u0026rsquo;analyse. Allons-y\nLa librairie pandas_datareader. Python a cet avantage qu\u0026rsquo;il existe beaucoup de librairies permettant de lire en temps réel les données financières. Pandas_datareader, Quantopian ou bien Quandl ne sont que quelques une des librairies qui permettent de faire ce travail. J\u0026rsquo;utiliserais cette première librairie pour cet article car elle est facile et fonctionne très bien avec Pandas.\n# Importation des librairies % matplotlib inline import pandas as pd from pandas_datareader import DataReader, wb import seaborn as sns import matplotlib.pyplot as plt  # Paramètres graphiques plt.rcParams[\u0026quot;figure.figsize\u0026quot;] = [12,6] plt.rcParams[\u0026quot;figure.dpi\u0026quot;] = 150 plt.style.use(\u0026quot;fivethirtyeight\u0026quot;)  Les librairies importées, passons maintenant à la lecture des informations financières. J\u0026rsquo;aimerais lire les informations de 3 informations. Une française, une américaine et une autre japonaise.\nCes entreprises ne sont rien d\u0026rsquo;autres que Total, Amazon et Toyota.\nJ\u0026rsquo;aimerais avoir leurs informations boursières d\u0026rsquo;il y a 10 ans à aujourd\u0026rsquo;hui 25 février 2019\ntotal = DataReader(\u0026quot;TOT\u0026quot;, data_source= \u0026quot;yahoo\u0026quot;, start= \u0026quot;2009-02-25\u0026quot;, end= \u0026quot;2019-02-25\u0026quot;) amazon = DataReader(\u0026quot;AMZN\u0026quot;, data_source= \u0026quot;yahoo\u0026quot;, start= \u0026quot;2009-02-25\u0026quot;, end= \u0026quot;2019-02-25\u0026quot;) toyota = DataReader(\u0026quot;TYO\u0026quot;, data_source= \u0026quot;yahoo\u0026quot;, start= \u0026quot;2009-02-25\u0026quot;, end= \u0026quot;2019-02-25\u0026quot;)  J\u0026rsquo;ai décidé de lire les informations à partir de Yahoo Finance. Chaque entreprise côtée en bourse a ce qu\u0026rsquo;on applle un \u0026lsquo;ticker\u0026rsquo;, c\u0026rsquo;est le premier argument que j\u0026rsquo;ai rentré dans la fonction DataReader : pour Total c\u0026rsquo;est TOT, Amazon, AMZN et Toyota, TYO. La liste de ces tickers est en ligne, si vous voulez trouver une entreprise particulière il faut juste taper sur Google , ticker \u0026ldquo;nom de l\u0026rsquo;entreprise\u0026rdquo;.\nMaintenant, regardons comment se présente chacune de ces données.\ntotal.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    High Low Open Close Volume Adj Close   Date           2009-02-25 49.730000 47.450001 49.000000 48.900002 5140800.0 28.170866   2009-02-26 48.959999 47.759998 48.160000 47.910000 4680700.0 27.600544   2009-02-27 48.360001 46.349998 46.759998 47.200001 3990300.0 27.191511   2009-03-02 46.189999 43.599998 45.810001 43.720001 4412600.0 25.186716   2009-03-03 44.680000 42.880001 44.680000 43.410000 3934000.0 25.008127     Pour chacune des entreprises ce sont ces informations que nous avons pour toute la période:\nLa date est lue comme l\u0026rsquo;index de la base de donnée, et le grand avantage c\u0026rsquo;est qu\u0026rsquo;on a une donnée Pandas donc on peut utiliser toute la librairie pour analyser et extraire de l\u0026rsquo;information à partir d\u0026rsquo;elle.\nLes colonnes High et Low représentent le prix le plus haut de l\u0026rsquo;action au jour. Open et Close sont les prix d\u0026rsquo;ouverture et de fermture ce même jour. Volume représente la quantité totale d\u0026rsquo;actions échangée échangée le jour.\nPour pouvoir analyser ensemble ces données concernant les 3 entreprises on peut les joindre ensemble dans un seul base de donnée.\ntickers = [\u0026quot;TOT\u0026quot;, \u0026quot;AMZN\u0026quot;, \u0026quot;TYO\u0026quot;]  entreprises = pd.concat([total, amazon, toyota], axis = 1, keys= tickers)  entreprises.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } .dataframe thead tr:last-of-type th { text-align: right; }     TOT AMZN TYO    High Low Open Close Volume Adj Close High Low Open Close Volume Adj Close High Low Open Close Volume Adj Close   Date                       2009-02-25 49.730000 47.450001 49.000000 48.900002 5140800.0 28.170866 65.750000 62.820000 64.900002 63.709999 9122100 63.709999 NaN NaN NaN NaN NaN NaN   2009-02-26 48.959999 47.759998 48.160000 47.910000 4680700.0 27.600544 64.730003 62.340000 64.120003 62.340000 7275300 62.340000 NaN NaN NaN NaN NaN NaN   2009-02-27 48.360001 46.349998 46.759998 47.200001 3990300.0 27.191511 65.080002 60.939999 61.259998 64.790001 11488300 64.790001 NaN NaN NaN NaN NaN NaN   2009-03-02 46.189999 43.599998 45.810001 43.720001 4412600.0 25.186716 65.519997 61.509998 63.939999 61.990002 10511900 61.990002 NaN NaN NaN NaN NaN NaN   2009-03-03 44.680000 42.880001 44.680000 43.410000 3934000.0 25.008127 63.290001 61.299999 62.750000 61.700001 9691600 61.700001 NaN NaN NaN NaN NaN NaN     En combinant les informations ensemble, on observe toute de suite que les données concernant Toyota en 2009 sont manquantes. Il semblerait donc que le groupe n\u0026rsquo;ait entré e bourse que plus tard. Regardons à partir de quel moment nous avons les informations le concernant.\nentreprises[\u0026quot;TYO\u0026quot;][\u0026quot;Open\u0026quot;].first_valid_index()  Timestamp('2009-04-16 00:00:00')  C\u0026rsquo;est donc à partir du 16 avril 2009 qu\u0026rsquo;on commence à trouver des informations disponibles concernant Toyota.\nVisualisation Evolution des prix plus haut tickers  ['TOT', 'AMZN', 'TYO']  for i, tick in enumerate(tickers): entreprises[tick][\u0026quot;Open\u0026quot;].plot() plt.legend(tickers)  La croissance des prix d\u0026rsquo;ouverture des actions Amazon est tellement fulgurante que sur l\u0026rsquo;echelle les deux autres prix sont mal représentés.\nOn peut zoomer sur ce graphique en ajustant l\u0026rsquo;echelle à un echelle logarithmique pour observer l\u0026rsquo;évolution des données avec le temps.\nimport numpy as np  for i, tick in enumerate(tickers): np.log(entreprises[tick][\u0026quot;Open\u0026quot;]).plot() plt.legend(tickers) plt.title(\u0026quot; Evolution du logarithme des prix d'ouverture\u0026quot;) plt.ylabel(\u0026quot;Prix en logarithme\u0026quot;);  plt.figure(figsize = (10,14)) for i, tick in enumerate(tickers): plt.subplot(3,1, i+1) plt.plot(entreprises[tick][\u0026quot;High\u0026quot;]) plt.title(f\u0026quot;High price for {tick}\u0026quot;) plt.tight_layout(h_pad = 2.5)  On observe pour ces trois entreprises que les variations des prix des actions diffèrent considérablement. Chez Total on observe une forte volatilité et une sorte de saisonalité, chez Amazon c\u0026rsquo;est une croissance fulgurante qu\u0026rsquo;on observe, depuis 2009 les prix ne cessent de grimper jusqu\u0026rsquo;à atteindre leur maximum mi 2018 pour baisser légrement ensuite. Quand à Toyota sur toute la période les prix ne font que baisser. Essayons de zoomer davantage sur ces données et considérons un horizon temporelle plus court : entre 2015 et 2019.\nA partir de 2015 from_2015 = entreprises.loc[\u0026quot;2015\u0026quot;:,]  plt.figure(figsize = (10,14)) for i, tick in enumerate(tickers): plt.subplot(3,1, i+1) plt.plot(from_2015[tick][\u0026quot;High\u0026quot;]) plt.title(f\u0026quot;High price for {tick}\u0026quot;) plt.tight_layout(h_pad = 2.5)  Voilà donc pour cet article ! Mon but était principalement de vous présenter comment il est possible de lire les informations financières en ligne. Les possibilité d\u0026rsquo;analyse sont nombreuses, on aura pu estimer et prévoir la volatilité de ces prix avec des modèles de type GARCH mais laissons cela pour une prochaine série !\n","date":1551052800,"expirydate":-62135596800,"kind":"page","lang":"fr","lastmod":1551052800,"objectID":"ecea238c314dfc94a14a715d5dcc9755","permalink":"https://agailloty.rbind.io/fr/post/financial-data-python/","publishdate":"2019-02-25T00:00:00Z","relpermalink":"/fr/post/financial-data-python/","section":"post","summary":"Utiliser Pandas-datareader pour lire les prix des actions","tags":["Python","stock data","analysis"],"title":"Lire les données financières","type":"post"},{"authors":["Axel-Cleris Gailloty"],"categories":["Python"],"content":" Utiliser Python pour explorer vos données Python est un langage généraliste, facile à utiliser et très puissant. Aujourd\u0026rsquo;hui de nombreuses applications et sites webs comme Dropbox, Youtube, Google tournent grâce à du code écrit en Python. C\u0026rsquo;est aussi à présent le langage qui bat les records en terme en matière d\u0026rsquo;adoption et de taux de croissance.\nPeu de temps après sa création, la communauté scientifique s\u0026rsquo;y est intéressée, ce qui a condit à l\u0026rsquo;écriture de la librairie scientifique numpy, dès ce jour l\u0026rsquo;avenir du langage a changé car l\u0026rsquo;écriture de cette librairie a posé la base sur laquelle l\u0026rsquo;écosystème scientifique et de la data science tourne.\nMon but dans cet article est de vous présenter brièvement l\u0026rsquo;écosytème data science en Python en réalisant une analyse de données. Je suis bien content !\nImportation des libraries # Nous commençons par importer les libraries que nous utiliserons tout au long de cette analyse import pandas as pd # librairie pour lecture et manipulation de donnée import numpy as np # notre fameuse numpy pour le calcul numérique import matplotlib.pyplot as plt # pour les visualisation  Dans cet article je vais travailler sur une base de donnée qui porte sur le classement de Shangai des Universités mondiales. La base de donnée a été librement distribuée par l\u0026rsquo;Organisme du classement et je l\u0026rsquo;ai eu grâce à la plateforme Kaggle.\nLecture de la base de donnée classement = pd.read_csv(\u0026quot;cwurData.csv\u0026quot;)  La base vient d\u0026rsquo;être lue en utilisant la librairie Pandas. Comme à mon habitude j\u0026rsquo;aime bien rapidement regarder les premmières lignes d\u0026rsquo;une base de donnée pour me faire une représentation visuelle quand je travaillerai sur elle.\nclassement.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    world_rank institution country national_rank quality_of_education alumni_employment quality_of_faculty publications influence citations broad_impact patents score year     0 1 Harvard University USA 1 7 9 1 1 1 1 NaN 5 100.00 2012   1 2 Massachusetts Institute of Technology USA 2 9 17 3 12 4 4 NaN 1 91.67 2012   2 3 Stanford University USA 3 17 11 5 4 2 2 NaN 15 89.50 2012   3 4 University of Cambridge United Kingdom 1 10 24 4 16 16 11 NaN 50 86.17 2012   4 5 California Institute of Technology USA 4 2 29 7 37 22 22 NaN 18 85.21 2012     classement.shape  (2200, 14)  La base de donné contient donc 2200 observations et 14 variables. La lecture des premières lignes de la base de donnée nous donne une indication que ces classents se sont faits sur plusieurs années, donc il est probable que certaines universités reviennent plusieurs fois.\nNous allons dans la suite de ce projet détermine quelles sont les uniques occurences des universités et combien d\u0026rsquo;universités au total nous avons dans la base.\nclassement = classement.set_index(\u0026quot;world_rank\u0026quot;)  On peut maintenant se demander s\u0026rsquo;il y a des données manquantes dans ce classement? Et si oui, où sont-elles localisées? La librairie Pandas nous offre des outils très utiles pour répondre à cette question et permet même de corriger les données manquantes grâce à des algorithmes puissants.\nclassement.isna().sum()  institution 0 country 0 national_rank 0 quality_of_education 0 alumni_employment 0 quality_of_faculty 0 publications 0 influence 0 citations 0 broad_impact 200 patents 0 score 0 year 0 dtype: int64  Cette commande nous montre que la plupart des variables de la base de donnée ne contient pas de données manquantes, seule la variable broad_impact en contient, et 200 précisément!\nSignification des libellés world_rank : classement mondial de l\u0026rsquo;Université\ninstitution Nom de l\u0026rsquo;Université\ncountry : Le pays dans lequel se trouve l\u0026rsquo;Université\nnational_rank: classement de l\u0026rsquo;Université au niveau national\nquality_of_education: rang en fonction de la qualité de l\u0026rsquo;éducation\nalumni_employment: Rang de l\u0026rsquo;Université en fonction de ses anciens étudiants\nquality_of_faculty: Rang en fonction de la qualité de la Faculté\npublications: Rang en fonction des publications (plus l\u0026rsquo;Université publie, mieux elle est classée)\ninfluence: Rang en fonction de l\u0026rsquo;influence\ncitations: Nombre des étudiants à l\u0026rsquo;Université\nbroad_impact: Impact général (only available for 2014 and 2015)\npatents rank for patents : rang pour les brevets\nscore score total, utilisé pour déterminer le classement\nyear: année du classement (2012 à 2015)\nAyant maintenant ces libellés en tête, on peut explorer davantage la base de donnée. Les clarifications nous indiquent la variable broad_impact n\u0026rsquo;est disponible que pour les années 2014 et 2015, donc on peut la garder dans notre base et l\u0026rsquo;explorer plus tard.\nA partir d\u0026rsquo;ici on peut se demander combien d\u0026rsquo;uniques universités il y a dans la base puisque la base de donnée classe les universités sur 3 ans.\nclassement[\u0026quot;institution\u0026quot;].nunique()  1024  Pour répondre à la question précédente, nous avons donc 1024 différentes universités dans ce classement.\nCombien de pays avons nous dans la base?\nclassement[\u0026quot;country\u0026quot;].nunique()  59  Comme on peut le voir, tous les pays du monde ne sont pas représentés, toutes les 1024 Universités de ce classement appartiennent uniquement à ces seuls pays. On peut maintenant se demander quels sont les pays qui ont les plus d\u0026rsquo;universités dans le classement.\nAffichons les 6 premiers et les 6 derniers\nclassement[\u0026quot;country\u0026quot;].value_counts().head(6)  USA 573 China 167 Japan 159 United Kingdom 144 Germany 115 France 109 Name: country, dtype: int64  Les USA, à eux seuls comptent 573 universités soit plus de la moitié des Universités présentes dans le classement. La France figure parmi les premiers avec 109 universités !\nclassement[\u0026quot;country\u0026quot;].value_counts().tail(6)  Serbia 2 Lebanon 2 Uganda 2 Puerto Rico 2 United Arab Emirates 2 Bulgaria 2 Name: country, dtype: int64  On peut aussi voir la distribution de ces universités graphiquement pour avoir une idée plus claire.\nplt.style.use('fivethirtyeight') # Thème  plt.figure(figsize = (12,6), dpi= 100) # Pour gérer la taille de la figure classement[\u0026quot;country\u0026quot;].value_counts().plot(kind = \u0026quot;bar\u0026quot;) plt.title(\u0026quot;Nombre d'Universités par pays\u0026quot;) plt.xlabel(\u0026quot;Pays\u0026quot;) plt.show()  Intéressons-nous maintenant à la distibution des scores. Les scores vont de 0 à 100\nplt.figure(figsize = (12,6), dpi= 100) # Pour gérer la taille de la figure classement[\u0026quot;score\u0026quot;].hist(bins = 50) plt.title(\u0026quot;Score des Universités\u0026quot;) plt.xlabel(\u0026quot;Nombre d'universités\u0026quot;) plt.show()  Une très grande partie des universités a un score compris entre 25 et 50. Qu\u0026rsquo;est-ce qui peut expliquer cela? Et quelles sont les universités ayant les meilleurs scores?\nRappellons-nous que les classements sont faits durant 3 années consécutives, donc il se peut qu\u0026rsquo;une université change de rang pendant ces années. C\u0026rsquo;est ce que nous allons essayer de voir\nOn peut se demander ici, combiens de classement il y a eu par année !\nclassement[\u0026quot;year\u0026quot;].value_counts()  2015 1000 2014 1000 2013 100 2012 100 Name: year, dtype: int64  On observe que clairement, les années diffèrent. En 2012 et 2013 seules 100 universités ont été classées. C\u0026rsquo;est à partir de 2014 que le classement s\u0026rsquo;est élargi à un plus grand nombre d\u0026rsquo;universités, 9 fois plus. Pourquoi cela?\nSaut Comme nous l\u0026rsquo;avons remarqué plus haut, le classement se fait sur 4 années consécutives, il serait donc utile de compter le nombre d\u0026rsquo;uniques occurences sur ces 4 années.\nfor cols, title in enumerate(classement.columns): print(f\u0026quot; Il y a {classement[title].nunique()} valeurs uniques dans, {title}\u0026quot;)   Il y a 1024 valeurs uniques dans, institution Il y a 59 valeurs uniques dans, country Il y a 229 valeurs uniques dans, national_rank Il y a 367 valeurs uniques dans, quality_of_education Il y a 565 valeurs uniques dans, alumni_employment Il y a 199 valeurs uniques dans, quality_of_faculty Il y a 987 valeurs uniques dans, publications Il y a 944 valeurs uniques dans, influence Il y a 135 valeurs uniques dans, citations Il y a 343 valeurs uniques dans, broad_impact Il y a 738 valeurs uniques dans, patents Il y a 764 valeurs uniques dans, score Il y a 4 valeurs uniques dans, year  series = pd.Series() for cols, title in enumerate(classement.columns): series[title] = classement[title].nunique()  plt.figure(figsize = (10,4), dpi= 100) # Pour gérer la taille de la figure series.plot(kind = \u0026quot;bar\u0026quot;, )  \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0x26ad3cfe470\u0026gt;  Que nous revèlent ces valeurs uniques?\nNous avons un total de 1024 universités dans la base de donné. Le classement mondial va de 1 à 1000. On peut donc déduire qu\u0026rsquo;il y a des universités qui sont classées plusieurs fois, durant les 4 années successives et également qu\u0026rsquo;il y a en même temps certaines universités qui sont sorties du classement pendant que d\u0026rsquo;autres entrent dans le classement, c\u0026rsquo;est ce qui explique le différentiel 1024 universités et 1000 classements. Même constat pour les variables publications et influence qui avoisinent les 1000 valeurs uniques.\nLa question qu\u0026rsquo;on se pose maintenant est de comprendre comment se fait-il qu\u0026rsquo;il y a si peu de valeurs uniques dans les autres variables servant de classement? Y\u0026rsquo;aurait-il des chevauchements, où y\u0026rsquo;aurait-il des universités ex-aequo sur certains classements?\nPour répondre à cette question, nous allons voir dans un tableau année par année la variation des uniques occurences.\nseries = pd.Series() unique = pd.DataFrame() years = list(classement[\u0026quot;year\u0026quot;].unique()) for year in years: mask = classement.loc[:,\u0026quot;year\u0026quot;] == year df = classement[mask] for cols, title in enumerate(df.columns): series[title] = df[title].nunique() unique = unique.append(series, ignore_index = True)  unique   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    alumni_employment broad_impact citations country influence institution national_rank patents publications quality_of_education quality_of_faculty score year     0 50.0 0.0 81.0 16.0 81.0 100.0 58.0 51.0 80.0 67.0 78.0 99.0 1.0   1 42.0 0.0 83.0 18.0 76.0 100.0 57.0 41.0 79.0 58.0 79.0 96.0 1.0   2 473.0 215.0 63.0 59.0 899.0 1000.0 229.0 71.0 919.0 354.0 176.0 435.0 1.0   3 564.0 211.0 61.0 59.0 915.0 1000.0 229.0 736.0 924.0 367.0 176.0 416.0 1.0     Dans ce tableau, nous avons les valeurs uniques année par année de toutes les colonnes de notre base de donnée.\nLes deux premières lignes représentent les années 2012 et 2013, pour ces années on observe que les valeurs uniques sont faibles car le classement s\u0026rsquo;effectuait sur un total de 100 pays seulement. Nous allons nous intéresser uniquement aux colonnes significatives : en 2012 il y a eu le classement en fonction des publication comptait 80 classements, ce nombre a baissé à 79 en 2013.\nLes deux autres colonnes représentent les années 2014 et 2015, là aussi on voit les évolutions des valeurs uniques.\nLes Universités françaises On peut maintenant s\u0026rsquo;intéresser aux Universités françaises, pour voir leurs caractéristiques.\nfrance = classement[\u0026quot;country\u0026quot;] == \u0026quot;France\u0026quot; univ_france = classement[france]  univ_france.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    institution country national_rank quality_of_education alumni_employment quality_of_faculty publications influence citations broad_impact patents score year   world_rank                  48 University of Paris-Sud France 1 48 101 25 73 96 101 NaN 101 50.44 2012   54 École normale supérieure - Paris France 2 3 61 77 101 101 101 NaN 101 49.10 2012   61 École Polytechnique France 3 47 3 101 101 101 101 NaN 101 48.33 2012   63 Pierre-and-Marie-Curie University France 4 54 101 84 36 53 78 NaN 101 48.26 2012   100 Mines ParisTech France 5 44 4 101 101 101 101 NaN 101 43.36 2012     Score des Universités françaises plt.figure(figsize = (10,4), dpi= 100) # Pour gérer la taille de la figure univ_france[\u0026quot;score\u0026quot;].plot.hist(bins = 30) plt.title(\u0026quot;Score des Universités françaises selon le classement de Shangai\u0026quot;) plt.xlabel(\u0026quot;Score sur 100\u0026quot;) plt.show()  On observe que les scores des Universités françaises se situent dans l\u0026rsquo;intervalle [40, 60]. Toutefois la plupart d\u0026rsquo;entre elles ont un score inférieur à 50.\nQuelles sont les universités 5 premières et dernières universités françaises selon ce classement? On ne s\u0026rsquo;intéresse qu\u0026rsquo;à l\u0026rsquo;année 2015\nuniv_france = univ_france  univ_france[univ_france[\u0026quot;year\u0026quot;] == 2015][[\u0026quot;institution\u0026quot;, \u0026quot;national_rank\u0026quot;, \u0026quot;score\u0026quot;]].nlargest(5, \u0026quot;score\u0026quot;)   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    institution national_rank score   world_rank        36 École Polytechnique 1 59.20   37 École normale supérieure - Paris 2 58.80   58 University of Paris-Sud 3 54.21   63 Pierre-and-Marie-Curie University 4 53.79   104 Mines ParisTech 5 50.34     univ_france[univ_france[\u0026quot;year\u0026quot;] == 2015][[\u0026quot;institution\u0026quot;, \u0026quot;national_rank\u0026quot;, \u0026quot;score\u0026quot;]].nsmallest(5, \u0026quot;score\u0026quot;)   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    institution national_rank score   world_rank        989 University of Pau and Pays de l'Adour 49 44.04   978 University of Reims Champagne-Ardenne 48 44.05   966 University of Valenciennes and Hainaut-Cambresis 47 44.06   896 University of Western Brittany 46 44.13   879 University of Orléans 43 44.15     La première Université française est classée 36e sur le rang mondial tandis que l\u0026rsquo;Université française dernière du classement national se positionne à la 989e place mondiale selon le classement Shangaï en 2015.\nC\u0026rsquo;est quoi les contrastes?\n   ","date":1549497600,"expirydate":-62135596800,"kind":"page","lang":"fr","lastmod":1549497600,"objectID":"5b787e5685b3de8cb3f5fabf30d9c6c2","permalink":"https://agailloty.rbind.io/fr/post/analyse-python/","publishdate":"2019-02-07T00:00:00Z","relpermalink":"/fr/post/analyse-python/","section":"post","summary":"Utiliser les librairies scientifiques de Python pour analyser un jeu de données.","tags":["Python","webscrape","analysis"],"title":"Analyser ses données avec Python","type":"post"},{"authors":[],"categories":[],"content":" Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = \\;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \nA fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears  Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view \r Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links   night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links  Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"fr","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://agailloty.rbind.io/fr/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/fr/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"}]